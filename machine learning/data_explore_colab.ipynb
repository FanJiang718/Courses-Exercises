{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pECGqVlyEyYb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "pd.options.display.width = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "rizhzGNsFP3J",
    "outputId": "70fd02c9-1fbe-458c-b4ef-56ea8fe9278c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ypdRgK5LGQJQ",
    "outputId": "ad84a097-86ff-4e98-fcbf-faec3c57fce2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'gdrive', 'sample_data']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = \"/content/gdrive/My Drive/Colab Notebooks/m2-proba-finance-2019\"\n",
    "os.listdir(\"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3cNZ-vk7EyYp"
   },
   "outputs": [],
   "source": [
    "D_author = pd.read_csv( path+\"/authorData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_author = pd.read_csv(\"authorData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Iay23yvREyYx",
    "outputId": "fac73d1c-9144-450f-a078-ebdc209c8c34",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['engagement', 'timestamp', 'language', 'feature1', 'feature2',\n",
       "       'followers', 'author', 'word_count', 'shared_url_count',\n",
       "       'shared_url_domain', 'is_reply', 'is_retweet', 'contains_video',\n",
       "       'contains_image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = D_author.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bKbyyJeyEyY5",
    "outputId": "c6486039-173e-4109-c459-5567a21c8529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(629877, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_author.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "rkBN377AEyY_",
    "outputId": "88f6e442-c612-4527-c8d1-36b3394fe561"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engagement</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>author</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>shared_url_domain</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>contains_video</th>\n",
       "      <th>contains_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1655713481254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>d2e03f4b7a57a6828392a80b6d8667c3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1655743160254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>5622c71671e9b0ab9052fd574876b46d</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3486</td>\n",
       "      <td>1655395750254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>-5</td>\n",
       "      <td>454592</td>\n",
       "      <td>9b72b401edaf6aec6603856a3a6fb050</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1654621088254</td>\n",
       "      <td>ar</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>19348</td>\n",
       "      <td>dc3d6148b435c40a4e75e7d6b5bd22c9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1655754531254</td>\n",
       "      <td>en</td>\n",
       "      <td>45</td>\n",
       "      <td>-5</td>\n",
       "      <td>973</td>\n",
       "      <td>0e47c55a7918dd9130da37a80e46a2e0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   engagement      timestamp language  feature1  feature2  followers  \\\n",
       "0           0  1655713481254       en       100         0        999   \n",
       "1           0  1655743160254       en       100        -5          5   \n",
       "2        3486  1655395750254       en       100        -5     454592   \n",
       "3           7  1654621088254       ar       100         5      19348   \n",
       "4           0  1655754531254       en        45        -5        973   \n",
       "\n",
       "                             author  word_count  shared_url_count  \\\n",
       "0  d2e03f4b7a57a6828392a80b6d8667c3          25                 0   \n",
       "1  5622c71671e9b0ab9052fd574876b46d           9                 0   \n",
       "2  9b72b401edaf6aec6603856a3a6fb050          45                 0   \n",
       "3  dc3d6148b435c40a4e75e7d6b5bd22c9          10                 0   \n",
       "4  0e47c55a7918dd9130da37a80e46a2e0          14                 0   \n",
       "\n",
       "  shared_url_domain  is_reply  is_retweet  contains_video  contains_image  \n",
       "0               NaN     False        True            True           False  \n",
       "1               NaN     False       False           False           False  \n",
       "2               NaN     False       False           False           False  \n",
       "3               NaN     False       False           False           False  \n",
       "4               NaN     False        True           False            True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "f6ztYEemEyZD"
   },
   "outputs": [],
   "source": [
    "D_train = pd.read_csv(path+\"/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "AZnYjXNzEyZF",
    "outputId": "84f8ceca-11a1-4fed-e63f-a773c67b7b38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engagement</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>author</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>shared_url_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>V1015</th>\n",
       "      <th>V1016</th>\n",
       "      <th>V1017</th>\n",
       "      <th>V1018</th>\n",
       "      <th>V1019</th>\n",
       "      <th>V1020</th>\n",
       "      <th>V1021</th>\n",
       "      <th>V1022</th>\n",
       "      <th>V1023</th>\n",
       "      <th>V1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1655784230254</td>\n",
       "      <td>en</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>696</td>\n",
       "      <td>bd20432d80dfe4825a7a106312bda52e</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066302</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.055845</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.047829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1655789734254</td>\n",
       "      <td>en</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1268</td>\n",
       "      <td>1016de41efe1384efac491c1514da67c</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>http://fllwrs.com/</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028814</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>-0.000962</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.018320</td>\n",
       "      <td>0.033285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91007</td>\n",
       "      <td>1655599613254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>17526</td>\n",
       "      <td>71e2ecc6cc1d6515a6b122fab4c63bfc</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020109</td>\n",
       "      <td>0.014174</td>\n",
       "      <td>0.019271</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>-0.001049</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>0.034822</td>\n",
       "      <td>0.042102</td>\n",
       "      <td>0.020760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1655788872254</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>239</td>\n",
       "      <td>2a9109f38d90a2a96e284a07c1a57e9b</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069541</td>\n",
       "      <td>0.094191</td>\n",
       "      <td>-0.002353</td>\n",
       "      <td>0.030343</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.007801</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.020304</td>\n",
       "      <td>0.032962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1655788810254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>-5</td>\n",
       "      <td>21531</td>\n",
       "      <td>efea6800044f4b9b1df1d74031b4e027</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.022897</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>0.031689</td>\n",
       "      <td>0.018264</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>0.024227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1038 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   engagement      timestamp language  feature1  feature2  followers  \\\n",
       "0          15  1655784230254       en        73         0        696   \n",
       "1           0  1655789734254       en        58         0       1268   \n",
       "2       91007  1655599613254       en       100         5      17526   \n",
       "3           3  1655788872254       en         0        -5        239   \n",
       "4          11  1655788810254       en       100        -5      21531   \n",
       "\n",
       "                             author  word_count  shared_url_count  \\\n",
       "0  bd20432d80dfe4825a7a106312bda52e          18                 0   \n",
       "1  1016de41efe1384efac491c1514da67c          13                 1   \n",
       "2  71e2ecc6cc1d6515a6b122fab4c63bfc          15                 0   \n",
       "3  2a9109f38d90a2a96e284a07c1a57e9b           8                 0   \n",
       "4  efea6800044f4b9b1df1d74031b4e027          49                 0   \n",
       "\n",
       "    shared_url_domain  ...     V1015     V1016     V1017     V1018     V1019  \\\n",
       "0                 NaN  ...  0.066302 -0.002309  0.006296 -0.003366  0.055845   \n",
       "1  http://fllwrs.com/  ...  0.028814  0.009487 -0.000962  0.003516  0.001307   \n",
       "2                 NaN  ...  0.020109  0.014174  0.019271  0.011605 -0.001049   \n",
       "3                 NaN  ...  0.069541  0.094191 -0.002353  0.030343  0.004930   \n",
       "4                 NaN  ...  0.040541  0.022897  0.003639  0.002614 -0.000489   \n",
       "\n",
       "      V1020     V1021     V1022     V1023     V1024  \n",
       "0  0.002813  0.030930  0.040009  0.017094  0.047829  \n",
       "1 -0.000108  0.036496  0.020098  0.018320  0.033285  \n",
       "2  0.018731  0.008732  0.034822  0.042102  0.020760  \n",
       "3  0.000064  0.007801  0.006239  0.020304  0.032962  \n",
       "4  0.006744  0.031689  0.018264  0.010356  0.024227  \n",
       "\n",
       "[5 rows x 1038 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5cLBnW1YEyZI",
    "outputId": "07a68bd8-b8a4-4fe7-bc75-10479da3c5f4",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536, 1038)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sort(D_train.drop(cols, axis=1).apply(lambda x: x.corr(D_train.engagement)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.52478904e-02 -9.16575920e-02 -8.40366421e-02 -8.31602299e-02\n",
      " -8.02303082e-02 -7.63369470e-02 -7.24076306e-02 -7.16053356e-02\n",
      " -7.08228439e-02 -6.91140556e-02 -6.77888684e-02 -6.44377376e-02\n",
      " -6.25780610e-02 -5.97483128e-02 -5.88288260e-02 -5.88261377e-02\n",
      " -5.84180872e-02 -5.82638433e-02 -5.80358323e-02 -5.72896158e-02\n",
      " -5.69959519e-02 -5.69870149e-02 -5.67783791e-02 -5.65760521e-02\n",
      " -5.61632015e-02 -5.51836711e-02 -5.39807404e-02 -5.35680039e-02\n",
      " -5.27998348e-02 -5.17237730e-02 -5.13782420e-02 -5.13595593e-02\n",
      " -5.11753828e-02 -5.02604475e-02 -5.01277186e-02 -5.00123320e-02\n",
      " -4.93957606e-02 -4.80655238e-02 -4.79364089e-02 -4.72380188e-02\n",
      " -4.63299333e-02 -4.61385988e-02 -4.50218535e-02 -4.47017065e-02\n",
      " -4.46786985e-02 -4.44455207e-02 -4.43471026e-02 -4.42761507e-02\n",
      " -4.41073339e-02 -4.39022907e-02 -4.38226122e-02 -4.32012999e-02\n",
      " -4.31926250e-02 -4.31022478e-02 -4.23725229e-02 -4.23117340e-02\n",
      " -4.22211052e-02 -4.21854317e-02 -4.17861455e-02 -4.16844189e-02\n",
      " -4.13760618e-02 -4.10177882e-02 -4.06542837e-02 -4.04683438e-02\n",
      " -4.03416596e-02 -4.02341033e-02 -4.00950728e-02 -4.00167818e-02\n",
      " -3.99687107e-02 -3.98899791e-02 -3.96079469e-02 -3.87287353e-02\n",
      " -3.86754659e-02 -3.84599677e-02 -3.82745393e-02 -3.79645965e-02\n",
      " -3.75895634e-02 -3.71052315e-02 -3.70497862e-02 -3.68799197e-02\n",
      " -3.65668112e-02 -3.64546314e-02 -3.54282123e-02 -3.51326846e-02\n",
      " -3.50456180e-02 -3.50063155e-02 -3.49949536e-02 -3.49729994e-02\n",
      " -3.42949260e-02 -3.38532716e-02 -3.36754276e-02 -3.34875082e-02\n",
      " -3.34509345e-02 -3.34257949e-02 -3.34000599e-02 -3.32226335e-02\n",
      " -3.30180282e-02 -3.27271803e-02 -3.25490613e-02 -3.23767935e-02\n",
      " -3.23101828e-02 -3.22338985e-02 -3.18635382e-02 -3.08817719e-02\n",
      " -3.05768587e-02 -3.05096442e-02 -3.03706338e-02 -3.03039221e-02\n",
      " -3.02709331e-02 -3.02085894e-02 -3.00198087e-02 -2.99309153e-02\n",
      " -2.98858940e-02 -2.93557914e-02 -2.93538974e-02 -2.93494799e-02\n",
      " -2.85599043e-02 -2.84668534e-02 -2.83861147e-02 -2.80849197e-02\n",
      " -2.79297033e-02 -2.78219299e-02 -2.77354999e-02 -2.75005217e-02\n",
      " -2.74412056e-02 -2.74116581e-02 -2.73838018e-02 -2.73500649e-02\n",
      " -2.70606932e-02 -2.69345470e-02 -2.69223418e-02 -2.68588107e-02\n",
      " -2.67805352e-02 -2.65274016e-02 -2.63763345e-02 -2.63033029e-02\n",
      " -2.62469468e-02 -2.61952845e-02 -2.60222291e-02 -2.59979730e-02\n",
      " -2.56844953e-02 -2.55185547e-02 -2.54551370e-02 -2.52730780e-02\n",
      " -2.52342064e-02 -2.51918285e-02 -2.51275523e-02 -2.48986164e-02\n",
      " -2.45231564e-02 -2.44554975e-02 -2.44119232e-02 -2.40669152e-02\n",
      " -2.39467015e-02 -2.38761638e-02 -2.37535179e-02 -2.36016090e-02\n",
      " -2.34461184e-02 -2.33598050e-02 -2.33295698e-02 -2.32303125e-02\n",
      " -2.30275342e-02 -2.28855362e-02 -2.28268095e-02 -2.28127435e-02\n",
      " -2.26035263e-02 -2.24106985e-02 -2.23877667e-02 -2.23781467e-02\n",
      " -2.22474294e-02 -2.22473378e-02 -2.21599529e-02 -2.20224976e-02\n",
      " -2.20167061e-02 -2.18530120e-02 -2.16866441e-02 -2.15905489e-02\n",
      " -2.15825323e-02 -2.12309978e-02 -2.10374419e-02 -2.09286837e-02\n",
      " -2.05184655e-02 -2.05100483e-02 -2.04307031e-02 -2.04203600e-02\n",
      " -2.03758056e-02 -2.01642725e-02 -2.01418237e-02 -2.01395384e-02\n",
      " -2.00321285e-02 -1.98868106e-02 -1.97818043e-02 -1.97408191e-02\n",
      " -1.96351567e-02 -1.95928333e-02 -1.92031162e-02 -1.92018693e-02\n",
      " -1.87906845e-02 -1.87169040e-02 -1.85386914e-02 -1.85011736e-02\n",
      " -1.82855006e-02 -1.82637552e-02 -1.82524414e-02 -1.81889147e-02\n",
      " -1.80428852e-02 -1.79255199e-02 -1.78453300e-02 -1.77894206e-02\n",
      " -1.74361240e-02 -1.72703394e-02 -1.72297063e-02 -1.72232246e-02\n",
      " -1.71090165e-02 -1.70961199e-02 -1.69509326e-02 -1.67839884e-02\n",
      " -1.65528581e-02 -1.65434428e-02 -1.65159329e-02 -1.64237893e-02\n",
      " -1.61303425e-02 -1.61212648e-02 -1.58654803e-02 -1.57681184e-02\n",
      " -1.56080303e-02 -1.55060693e-02 -1.53564361e-02 -1.52502882e-02\n",
      " -1.52498863e-02 -1.52482003e-02 -1.50924207e-02 -1.49710880e-02\n",
      " -1.49660765e-02 -1.47774512e-02 -1.47678239e-02 -1.46645627e-02\n",
      " -1.45172137e-02 -1.44833479e-02 -1.42735139e-02 -1.38886099e-02\n",
      " -1.38766744e-02 -1.38695913e-02 -1.38378096e-02 -1.36936544e-02\n",
      " -1.36228159e-02 -1.36067634e-02 -1.35233822e-02 -1.34918620e-02\n",
      " -1.34543844e-02 -1.30482199e-02 -1.30349258e-02 -1.30084407e-02\n",
      " -1.26685312e-02 -1.26553750e-02 -1.26296460e-02 -1.25785792e-02\n",
      " -1.24896981e-02 -1.24390722e-02 -1.24319276e-02 -1.22158618e-02\n",
      " -1.22051070e-02 -1.21019835e-02 -1.20916359e-02 -1.19052842e-02\n",
      " -1.18360936e-02 -1.17897908e-02 -1.17784112e-02 -1.16816652e-02\n",
      " -1.15848392e-02 -1.14921917e-02 -1.14700865e-02 -1.13542838e-02\n",
      " -1.13479341e-02 -1.13389267e-02 -1.12392399e-02 -1.12048238e-02\n",
      " -1.11760034e-02 -1.11141624e-02 -1.10180507e-02 -1.09793412e-02\n",
      " -1.08664269e-02 -1.08255188e-02 -1.06828037e-02 -1.06749653e-02\n",
      " -1.05938298e-02 -1.05927440e-02 -1.04943067e-02 -1.04702305e-02\n",
      " -1.04234167e-02 -1.03569517e-02 -1.02395334e-02 -1.01901944e-02\n",
      " -1.01569690e-02 -1.00994425e-02 -9.63101796e-03 -9.58465979e-03\n",
      " -9.53873312e-03 -9.46870955e-03 -9.24311269e-03 -9.24259622e-03\n",
      " -9.21088469e-03 -8.97713819e-03 -8.69503730e-03 -8.57932644e-03\n",
      " -8.31047381e-03 -8.17764598e-03 -8.16347885e-03 -8.11418397e-03\n",
      " -7.99487291e-03 -7.75191108e-03 -7.69357682e-03 -7.68218904e-03\n",
      " -7.50079836e-03 -7.33351548e-03 -7.29265349e-03 -7.19315456e-03\n",
      " -7.14926670e-03 -7.08605429e-03 -7.06977425e-03 -7.04833848e-03\n",
      " -6.74578676e-03 -6.48870308e-03 -6.32437072e-03 -6.25627237e-03\n",
      " -6.14121703e-03 -6.11429423e-03 -6.04165386e-03 -5.97881309e-03\n",
      " -5.95925288e-03 -5.84123601e-03 -5.79471863e-03 -5.62954704e-03\n",
      " -5.56749505e-03 -5.53366096e-03 -5.46142945e-03 -5.37135222e-03\n",
      " -5.05853318e-03 -4.80897703e-03 -4.79468383e-03 -4.47077496e-03\n",
      " -4.40616352e-03 -4.29453154e-03 -4.20327261e-03 -4.13399369e-03\n",
      " -3.89354427e-03 -3.82930978e-03 -3.78470505e-03 -3.76721478e-03\n",
      " -3.63826304e-03 -3.53412714e-03 -3.53302761e-03 -3.48977844e-03\n",
      " -3.43589705e-03 -3.43556632e-03 -3.38772584e-03 -3.20031200e-03\n",
      " -3.16936996e-03 -3.09659857e-03 -3.01315265e-03 -2.99114911e-03\n",
      " -2.96725043e-03 -2.95080633e-03 -2.93988386e-03 -2.88964572e-03\n",
      " -2.85739118e-03 -2.82420716e-03 -2.82304362e-03 -2.76241604e-03\n",
      " -2.63908720e-03 -2.63134020e-03 -2.62050298e-03 -2.61066994e-03\n",
      " -2.40554144e-03 -2.29509854e-03 -2.22823739e-03 -2.21726534e-03\n",
      " -2.16607165e-03 -2.08576574e-03 -2.03487973e-03 -2.01737551e-03\n",
      " -1.98079089e-03 -1.92820764e-03 -1.91573922e-03 -1.74548520e-03\n",
      " -1.69771601e-03 -1.65566038e-03 -1.58582987e-03 -1.55165019e-03\n",
      " -1.24862864e-03 -1.23153598e-03 -1.21072373e-03 -1.16950809e-03\n",
      " -8.49356628e-04 -8.06065765e-04 -6.91584209e-04 -5.44509958e-04\n",
      " -4.60384483e-04 -2.46759674e-04 -2.03380078e-04 -1.87148394e-04\n",
      " -8.88083516e-05 -1.39201348e-06  6.30836798e-05  7.31941078e-05\n",
      "  1.22222152e-04  1.63895600e-04  1.75983733e-04  2.22581822e-04\n",
      "  4.51804571e-04  4.92479653e-04  4.92709294e-04  5.25482750e-04\n",
      "  6.00654370e-04  6.01381054e-04  7.67137204e-04  8.00151184e-04\n",
      "  8.06523042e-04  8.09141466e-04  9.12716459e-04  9.22419479e-04\n",
      "  1.27068366e-03  1.42219891e-03  1.44507353e-03  1.46198229e-03\n",
      "  1.46384038e-03  1.51395548e-03  1.56914435e-03  1.71902155e-03\n",
      "  1.74256418e-03  1.78495031e-03  1.87900753e-03  1.88075476e-03\n",
      "  1.89534585e-03  1.90738496e-03  1.98905638e-03  2.00569748e-03\n",
      "  2.01214828e-03  2.17952485e-03  2.28841366e-03  2.36143676e-03\n",
      "  2.40774038e-03  2.52618735e-03  2.58780528e-03  2.62501390e-03\n",
      "  2.68882397e-03  2.94015973e-03  2.98201250e-03  3.04893656e-03\n",
      "  3.06863435e-03  3.08794394e-03  3.24908184e-03  3.29219446e-03\n",
      "  3.48448304e-03  3.51816808e-03  3.64327952e-03  3.70238893e-03\n",
      "  3.78308548e-03  3.78327385e-03  4.08792538e-03  4.13996033e-03\n",
      "  4.18685334e-03  4.49008913e-03  4.68899994e-03  4.69207182e-03\n",
      "  4.70564840e-03  4.73391573e-03  4.76977047e-03  4.80589962e-03\n",
      "  4.81107396e-03  4.96490371e-03  5.09735534e-03  5.13259381e-03\n",
      "  5.21537331e-03  5.27546738e-03  5.28372331e-03  5.28707432e-03\n",
      "  5.29366813e-03  5.34318289e-03  5.36241793e-03  5.37211790e-03\n",
      "  5.37357217e-03  5.51486221e-03  5.54741314e-03  5.55604102e-03\n",
      "  5.56246529e-03  5.58873949e-03  5.77649758e-03  5.80928020e-03\n",
      "  5.84704542e-03  6.08030596e-03  6.13370747e-03  6.26723854e-03\n",
      "  6.27840428e-03  6.30644348e-03  6.35217753e-03  6.46035843e-03\n",
      "  6.52954115e-03  6.66368949e-03  6.74725140e-03  6.83598775e-03\n",
      "  6.86993435e-03  6.97099502e-03  7.06334517e-03  7.11869211e-03\n",
      "  7.18948271e-03  7.21471259e-03  7.22516169e-03  7.26451834e-03\n",
      "  7.35270216e-03  7.55291830e-03  7.58165205e-03  7.61192599e-03\n",
      "  7.73317871e-03  7.74580627e-03  7.75656647e-03  7.89124013e-03\n",
      "  7.94841615e-03  7.97441718e-03  7.99189744e-03  8.07864111e-03\n",
      "  8.16641564e-03  8.24108940e-03  8.38183848e-03  8.89833024e-03\n",
      "  8.89887278e-03  8.94662733e-03  8.96956234e-03  9.14635747e-03\n",
      "  9.31667127e-03  9.46118942e-03  9.63635323e-03  9.71545019e-03\n",
      "  9.77828980e-03  9.78864261e-03  9.87225453e-03  9.95640139e-03\n",
      "  1.01020619e-02  1.01069167e-02  1.01939845e-02  1.02394270e-02\n",
      "  1.02918590e-02  1.03406188e-02  1.03981517e-02  1.04754422e-02\n",
      "  1.04849604e-02  1.05821883e-02  1.06314834e-02  1.06575765e-02\n",
      "  1.07319549e-02  1.09231368e-02  1.09967256e-02  1.10249879e-02\n",
      "  1.12257959e-02  1.12897569e-02  1.13660812e-02  1.14208490e-02\n",
      "  1.15192916e-02  1.15592954e-02  1.16007847e-02  1.16097066e-02\n",
      "  1.16114331e-02  1.16432663e-02  1.16610516e-02  1.17050296e-02\n",
      "  1.17065169e-02  1.18701133e-02  1.19275255e-02  1.21770549e-02\n",
      "  1.24463742e-02  1.26340276e-02  1.26434284e-02  1.26811794e-02\n",
      "  1.27078079e-02  1.29517085e-02  1.29763415e-02  1.29978870e-02\n",
      "  1.30065826e-02  1.31077718e-02  1.32379523e-02  1.32643578e-02\n",
      "  1.33268666e-02  1.35927846e-02  1.36828597e-02  1.40671008e-02\n",
      "  1.41020932e-02  1.42690167e-02  1.42719916e-02  1.42933164e-02\n",
      "  1.46642040e-02  1.49870237e-02  1.50496429e-02  1.50529923e-02\n",
      "  1.51462068e-02  1.53898142e-02  1.54471269e-02  1.54683956e-02\n",
      "  1.55003425e-02  1.55281479e-02  1.56508948e-02  1.57634837e-02\n",
      "  1.60945457e-02  1.61001239e-02  1.61025529e-02  1.64649745e-02\n",
      "  1.65572238e-02  1.65966486e-02  1.70223171e-02  1.70280998e-02\n",
      "  1.73301720e-02  1.73796548e-02  1.74064762e-02  1.77006792e-02\n",
      "  1.77136785e-02  1.78286684e-02  1.78786924e-02  1.79277511e-02\n",
      "  1.80000299e-02  1.81716932e-02  1.83192758e-02  1.84374797e-02\n",
      "  1.84710575e-02  1.85725658e-02  1.86963883e-02  1.89298666e-02\n",
      "  1.91767921e-02  1.92754434e-02  1.93005014e-02  1.93567662e-02\n",
      "  1.93665682e-02  1.94653323e-02  1.95053581e-02  1.98167323e-02\n",
      "  2.00521437e-02  2.02129312e-02  2.02199283e-02  2.02665048e-02\n",
      "  2.02816822e-02  2.03080217e-02  2.03741541e-02  2.03795123e-02\n",
      "  2.03837457e-02  2.05194567e-02  2.05974399e-02  2.07215291e-02\n",
      "  2.08214337e-02  2.08827597e-02  2.09524934e-02  2.10911592e-02\n",
      "  2.11380224e-02  2.11948373e-02  2.13132303e-02  2.14103030e-02\n",
      "  2.14150201e-02  2.16355389e-02  2.21084824e-02  2.22072516e-02\n",
      "  2.23982938e-02  2.25914716e-02  2.27238190e-02  2.27291839e-02\n",
      "  2.28066535e-02  2.28238621e-02  2.29194468e-02  2.34232105e-02\n",
      "  2.35685370e-02  2.37277268e-02  2.37649031e-02  2.39504492e-02\n",
      "  2.42530767e-02  2.43671280e-02  2.46032989e-02  2.46305376e-02\n",
      "  2.50890516e-02  2.52092908e-02  2.57015070e-02  2.57027651e-02\n",
      "  2.58894047e-02  2.59331955e-02  2.62362396e-02  2.63370349e-02\n",
      "  2.63960260e-02  2.64018492e-02  2.66553435e-02  2.68213627e-02\n",
      "  2.68890408e-02  2.69947329e-02  2.70276692e-02  2.71210982e-02\n",
      "  2.72328285e-02  2.75994241e-02  2.77176536e-02  2.80160657e-02\n",
      "  2.80972676e-02  2.81009124e-02  2.82186367e-02  2.83204735e-02\n",
      "  2.83537672e-02  2.84474354e-02  2.86698022e-02  2.87946799e-02\n",
      "  2.88620400e-02  2.89972801e-02  2.90139962e-02  2.90585081e-02\n",
      "  2.91596679e-02  2.92916134e-02  2.92974199e-02  2.92977565e-02\n",
      "  2.93311892e-02  2.93734964e-02  2.93773863e-02  2.94346239e-02\n",
      "  2.98006378e-02  2.98025178e-02  2.98165412e-02  2.99449341e-02\n",
      "  2.99742401e-02  3.00540211e-02  3.01244730e-02  3.01415708e-02\n",
      "  3.01669639e-02  3.02009622e-02  3.03623927e-02  3.04096682e-02\n",
      "  3.05628717e-02  3.06733279e-02  3.06813909e-02  3.07327847e-02\n",
      "  3.07769652e-02  3.08386390e-02  3.09451822e-02  3.09691278e-02\n",
      "  3.11729407e-02  3.14573468e-02  3.15327389e-02  3.15677072e-02\n",
      "  3.16313371e-02  3.16519364e-02  3.16704887e-02  3.16910631e-02\n",
      "  3.18292510e-02  3.19801238e-02  3.20493030e-02  3.24274601e-02\n",
      "  3.24581782e-02  3.25538088e-02  3.26870983e-02  3.27078770e-02\n",
      "  3.28502349e-02  3.30383252e-02  3.30488787e-02  3.30887399e-02\n",
      "  3.32815895e-02  3.32942574e-02  3.35201633e-02  3.36676993e-02\n",
      "  3.38697808e-02  3.39177015e-02  3.39511782e-02  3.42249179e-02\n",
      "  3.48602645e-02  3.50450787e-02  3.50630868e-02  3.50922620e-02\n",
      "  3.52728578e-02  3.54881639e-02  3.55774270e-02  3.56537818e-02\n",
      "  3.57047041e-02  3.58179626e-02  3.59349950e-02  3.59373724e-02\n",
      "  3.59829490e-02  3.61452382e-02  3.61784810e-02  3.65942034e-02\n",
      "  3.68266478e-02  3.70205136e-02  3.70439549e-02  3.70859319e-02\n",
      "  3.74802159e-02  3.75284105e-02  3.77614829e-02  3.77724712e-02\n",
      "  3.78345247e-02  3.79213562e-02  3.84851590e-02  3.86494059e-02\n",
      "  3.88362505e-02  3.96138553e-02  3.97045448e-02  3.97408160e-02\n",
      "  3.97522481e-02  4.04837066e-02  4.05340731e-02  4.06394796e-02\n",
      "  4.08877906e-02  4.11256529e-02  4.15314722e-02  4.15946832e-02\n",
      "  4.21682620e-02  4.26189844e-02  4.28334321e-02  4.31752455e-02\n",
      "  4.32863204e-02  4.32863698e-02  4.34332717e-02  4.36028736e-02\n",
      "  4.36341006e-02  4.39839668e-02  4.41813741e-02  4.41816471e-02\n",
      "  4.43495991e-02  4.45176693e-02  4.47753155e-02  4.48298131e-02\n",
      "  4.51651502e-02  4.52194665e-02  4.53436357e-02  4.53633170e-02\n",
      "  4.56606008e-02  4.57375808e-02  4.61504504e-02  4.71019102e-02\n",
      "  4.72308323e-02  4.75701948e-02  4.79977108e-02  4.89493768e-02\n",
      "  4.90880667e-02  4.91134678e-02  4.92365520e-02  4.96978950e-02\n",
      "  4.97038391e-02  4.97922220e-02  4.98695768e-02  4.99156209e-02\n",
      "  5.01890880e-02  5.01901040e-02  5.03537361e-02  5.04896781e-02\n",
      "  5.08088571e-02  5.12175738e-02  5.12639142e-02  5.13822026e-02\n",
      "  5.16507056e-02  5.18247776e-02  5.22105451e-02  5.27123157e-02\n",
      "  5.32346192e-02  5.32618794e-02  5.36821201e-02  5.37419453e-02\n",
      "  5.39139172e-02  5.42287794e-02  5.43188437e-02  5.44305196e-02\n",
      "  5.48431575e-02  5.51347411e-02  5.52259836e-02  5.52742402e-02\n",
      "  5.53707458e-02  5.55922707e-02  5.57230868e-02  5.67511847e-02\n",
      "  5.68280231e-02  5.69546340e-02  5.70527262e-02  5.71084898e-02\n",
      "  5.71557881e-02  5.72167708e-02  5.73442856e-02  5.83212293e-02\n",
      "  5.83545687e-02  5.83729619e-02  5.86068132e-02  5.94497129e-02\n",
      "  5.97010928e-02  5.98058521e-02  5.98182986e-02  6.00062971e-02\n",
      "  6.03165044e-02  6.03211043e-02  6.04779804e-02  6.07480803e-02\n",
      "  6.07571334e-02  6.09631766e-02  6.13302674e-02  6.18988716e-02\n",
      "  6.19998144e-02  6.23241700e-02  6.27195538e-02  6.29400107e-02\n",
      "  6.36193253e-02  6.37430901e-02  6.42217259e-02  6.44291766e-02\n",
      "  6.44839713e-02  6.52637275e-02  6.57949927e-02  6.65392269e-02\n",
      "  6.65492084e-02  6.68632394e-02  6.69410867e-02  6.77994357e-02\n",
      "  6.78090159e-02  6.78262208e-02  6.78861056e-02  6.81855704e-02\n",
      "  6.85029430e-02  6.87589212e-02  6.91154928e-02  6.91277255e-02\n",
      "  6.91507089e-02  6.95490438e-02  6.99562034e-02  7.02200530e-02\n",
      "  7.02251429e-02  7.03088649e-02  7.14385687e-02  7.19274940e-02\n",
      "  7.22170820e-02  7.23981770e-02  7.27106615e-02  7.27315963e-02\n",
      "  7.36125683e-02  7.40271271e-02  7.40416378e-02  7.41513921e-02\n",
      "  7.42368983e-02  7.52702726e-02  7.52941268e-02  7.59043632e-02\n",
      "  7.67897534e-02  7.70201957e-02  7.75345087e-02  7.77249491e-02\n",
      "  7.79719387e-02  7.85508262e-02  7.85926568e-02  7.95928072e-02\n",
      "  7.99137760e-02  8.01156044e-02  8.03232084e-02  8.16688074e-02\n",
      "  8.18619101e-02  8.19513464e-02  8.22331903e-02  8.30770556e-02\n",
      "  8.31244656e-02  8.31830528e-02  8.33752666e-02  8.41750677e-02\n",
      "  8.52731188e-02  8.57493351e-02  8.59034680e-02  8.79569459e-02\n",
      "  8.80246375e-02  8.81703039e-02  8.83765820e-02  9.06022813e-02\n",
      "  9.11123855e-02  9.21637768e-02  9.32339775e-02  9.49644619e-02\n",
      "  9.55563020e-02  9.58122873e-02  9.60509730e-02  9.65013946e-02\n",
      "  9.65433681e-02  9.68516605e-02  9.70402699e-02  1.00515446e-01\n",
      "  1.00678752e-01  1.01266827e-01  1.01279896e-01  1.01766679e-01\n",
      "  1.01771883e-01  1.02615251e-01  1.02890913e-01  1.03098529e-01\n",
      "  1.03363939e-01  1.04793928e-01  1.04938965e-01  1.05684176e-01\n",
      "  1.05711090e-01  1.06059170e-01  1.06659031e-01  1.07302584e-01\n",
      "  1.07377477e-01  1.07598400e-01  1.10973436e-01  1.12169833e-01\n",
      "  1.14740586e-01  1.15329066e-01  1.16832444e-01  1.17557095e-01\n",
      "  1.18152864e-01  1.20384147e-01  1.20872830e-01  1.22290935e-01\n",
      "  1.23334861e-01  1.23421470e-01  1.24624712e-01  1.25324605e-01\n",
      "  1.26915979e-01  1.33975659e-01  1.35541800e-01  1.35806904e-01\n",
      "  1.38622843e-01  1.39597936e-01  1.39788645e-01  1.40068229e-01\n",
      "  1.42204972e-01  1.44343026e-01  1.45179801e-01  1.48973940e-01\n",
      "  1.49942338e-01  1.50025366e-01  1.50103780e-01  1.50902451e-01\n",
      "  1.53606523e-01  1.56411579e-01  1.73608790e-01  1.89910433e-01]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1              -0.003124\n",
       "feature2              -0.009128\n",
       "followers              0.175782\n",
       "word_count             0.005364\n",
       "shared_url_count      -0.009132\n",
       "is_reply              -0.053136\n",
       "is_retweet            -0.046675\n",
       "contains_video         0.047084\n",
       "contains_image        -0.002648\n",
       "language_popularity    0.010528\n",
       "avg_engage_language    0.024721\n",
       "author_popularity     -0.080551\n",
       "avg_engage_author      0.597416\n",
       "url_popularity        -0.004891\n",
       "avg_engage_url         0.112488\n",
       "dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.apply(lambda x: x.corr(D_X.engagement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GoRhxPHEEyZK"
   },
   "outputs": [],
   "source": [
    "D_test = pd.read_csv(path+\"/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "2fO8FDiMEyZM",
    "outputId": "d2c14a56-ab06-49e9-a580-f14bab2c6ac5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>author</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>shared_url_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>V1015</th>\n",
       "      <th>V1016</th>\n",
       "      <th>V1017</th>\n",
       "      <th>V1018</th>\n",
       "      <th>V1019</th>\n",
       "      <th>V1020</th>\n",
       "      <th>V1021</th>\n",
       "      <th>V1022</th>\n",
       "      <th>V1023</th>\n",
       "      <th>V1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1655750787254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>-5</td>\n",
       "      <td>3562</td>\n",
       "      <td>6dab33d47b0dd5d030e207261f93c319</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.011511</td>\n",
       "      <td>0.005093</td>\n",
       "      <td>0.014162</td>\n",
       "      <td>0.012245</td>\n",
       "      <td>3.456288e-03</td>\n",
       "      <td>0.036079</td>\n",
       "      <td>0.043002</td>\n",
       "      <td>0.014374</td>\n",
       "      <td>0.016828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1655788783254</td>\n",
       "      <td>en</td>\n",
       "      <td>13</td>\n",
       "      <td>-5</td>\n",
       "      <td>13609</td>\n",
       "      <td>f4dfaa9431bb01891104048b8d2b2cba</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.035142</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>1.209493e-03</td>\n",
       "      <td>0.031409</td>\n",
       "      <td>0.046286</td>\n",
       "      <td>0.013817</td>\n",
       "      <td>0.037918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1655789562254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>49903</td>\n",
       "      <td>70b84c15b524cf19cfee948a1357d44a</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044783</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>6.022647e-07</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>0.033760</td>\n",
       "      <td>0.020529</td>\n",
       "      <td>0.044053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1655665563254</td>\n",
       "      <td>en</td>\n",
       "      <td>95</td>\n",
       "      <td>-5</td>\n",
       "      <td>29317</td>\n",
       "      <td>17489c483613cc56ad60d5c7c43b3746</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>http://wsj.com/</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>0.026362</td>\n",
       "      <td>0.035867</td>\n",
       "      <td>9.164630e-03</td>\n",
       "      <td>0.022843</td>\n",
       "      <td>0.045851</td>\n",
       "      <td>0.028039</td>\n",
       "      <td>0.021808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1655789291254</td>\n",
       "      <td>en</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "      <td>5bc44ed424919266ebf344081fdea1a8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.025770</td>\n",
       "      <td>-0.000291</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>1.361860e-03</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.036191</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.020059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1038 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id      timestamp language  feature1  feature2  followers  \\\n",
       "0   1  1655750787254       en       100        -5       3562   \n",
       "1   2  1655788783254       en        13        -5      13609   \n",
       "2   3  1655789562254       en       100         0      49903   \n",
       "3   4  1655665563254       en        95        -5      29317   \n",
       "4   5  1655789291254       en       100         0        154   \n",
       "\n",
       "                             author  word_count  shared_url_count  \\\n",
       "0  6dab33d47b0dd5d030e207261f93c319           6                 0   \n",
       "1  f4dfaa9431bb01891104048b8d2b2cba           7                 0   \n",
       "2  70b84c15b524cf19cfee948a1357d44a          12                 0   \n",
       "3  17489c483613cc56ad60d5c7c43b3746          22                 1   \n",
       "4  5bc44ed424919266ebf344081fdea1a8           3                 0   \n",
       "\n",
       "  shared_url_domain  ...     V1015     V1016     V1017     V1018     V1019  \\\n",
       "0               NaN  ...  0.001071  0.011511  0.005093  0.014162  0.012245   \n",
       "1               NaN  ...  0.060200  0.035142  0.017822  0.010709  0.002361   \n",
       "2               NaN  ...  0.044783 -0.001629 -0.001456  0.005705  0.003102   \n",
       "3   http://wsj.com/  ...  0.040958  0.013183 -0.002016  0.026362  0.035867   \n",
       "4               NaN  ...  0.018229  0.025770 -0.000291  0.001325 -0.000563   \n",
       "\n",
       "          V1020     V1021     V1022     V1023     V1024  \n",
       "0  3.456288e-03  0.036079  0.043002  0.014374  0.016828  \n",
       "1  1.209493e-03  0.031409  0.046286  0.013817  0.037918  \n",
       "2  6.022647e-07  0.032912  0.033760  0.020529  0.044053  \n",
       "3  9.164630e-03  0.022843  0.045851  0.028039  0.021808  \n",
       "4  1.361860e-03  0.020734  0.036191  0.006318  0.020059  \n",
       "\n",
       "[5 rows x 1038 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-_UMkZ_hEyZO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zs6J-0xpEyZQ"
   },
   "outputs": [],
   "source": [
    "Y = np.concatenate( [ D_author[\"engagement\"].to_numpy(), D_train[\"engagement\"].to_numpy() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yFA-0vKzEyZR"
   },
   "outputs": [],
   "source": [
    "X = pd.concat( [ D_author[cols[1:]], D_train[cols[1:]] ] )\n",
    "X.fillna(\"\",inplace=True)\n",
    "D_X = pd.concat( [ D_author[cols], D_train[cols] ] )\n",
    "D_X.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FMPRkVdEyZT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-g0FoRUkEyZU"
   },
   "outputs": [],
   "source": [
    "dict_language = dict(X[\"language\"].value_counts())\n",
    "language_engagement = {}\n",
    "def freq_count(row, col, dic):\n",
    "    dic[row[col]] = dic.get(row[col],0) + row[\"engagement\"]\n",
    "    return\n",
    "_= D_X.apply( freq_count ,axis=1, args=(\"language\",language_engagement))\n",
    "average_engage = {}\n",
    "for key, value in dict_language.items():\n",
    "    average_engage[key] = language_engagement[key]/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_lang_avg_pop = np.mean(list(dict_language.values()))\n",
    "global_avg_engage_lang = np.mean(list(average_engage.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Hwyn5Gd4EyZV"
   },
   "outputs": [],
   "source": [
    "X[\"language_popularity\"] = X.apply(lambda row: dict_language[row[\"language\"]],axis = 1)\n",
    "X[\"avg_engage_language\"] = X.apply(lambda row: average_engage[row[\"language\"]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CN-IaQfqEyZX"
   },
   "outputs": [],
   "source": [
    "dict_author = dict(X[\"author\"].value_counts())\n",
    "author_engagement = {}\n",
    "_= D_X.apply( freq_count ,axis=1, args=(\"author\",author_engagement))\n",
    "avg_engage_author = {}\n",
    "for key, value in dict_author.items():\n",
    "    avg_engage_author[key] = author_engagement[key]/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_author_avg_pop = np.mean(list(dict_author.values()))\n",
    "global_avg_engage_author = np.mean(list(avg_engage_author.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5PS3OdzpEyZZ"
   },
   "outputs": [],
   "source": [
    "X[\"author_popularity\"] = X.apply(lambda row: dict_author[row[\"author\"]],axis = 1)\n",
    "X[\"avg_engage_author\"] = X.apply(lambda row: avg_engage_author[row[\"author\"]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "llYvET-ZEyZb"
   },
   "outputs": [],
   "source": [
    "dict_url = dict(X[\"shared_url_domain\"].value_counts())\n",
    "url_engagement = {}\n",
    "_= D_X.apply( freq_count ,axis=1, args=(\"shared_url_domain\",url_engagement))\n",
    "avg_engage_url = {}\n",
    "for key, value in dict_url.items():\n",
    "    avg_engage_url[key] = url_engagement[key]/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LJwCoSuYEyZc"
   },
   "outputs": [],
   "source": [
    "X[\"has_url\"] = X.apply(lambda row: 0 if row[\"shared_url_domain\"] == \"\" else 1,axis = 1)\n",
    "X[\"url_popularity\"] = X.apply(lambda row: dict_url[row[\"shared_url_domain\"]] if row[\"has_url\"] else 0,axis = 1)\n",
    "X[\"avg_engage_url\"] = X.apply(lambda row: avg_engage_url[row[\"shared_url_domain\"]] if row[\"has_url\"] else 0,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8hr3KCc0EyZd"
   },
   "outputs": [],
   "source": [
    "boolean_cols = [\"is_reply\", \"is_retweet\", \"contains_video\", \"contains_image\"]\n",
    "X[boolean_cols] = X[boolean_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EvK7JVNqEyZf"
   },
   "outputs": [],
   "source": [
    "dropped_cols = [\"language\", \"author\", \"shared_url_domain\"]\n",
    "X.drop(labels=dropped_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "txOaJNaBEyZg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "jnlJlh2nEyZh",
    "outputId": "21ed5100-bd1a-4cbe-a5c7-87f65dbc3cf4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>contains_video</th>\n",
       "      <th>contains_image</th>\n",
       "      <th>language_popularity</th>\n",
       "      <th>avg_engage_language</th>\n",
       "      <th>author_popularity</th>\n",
       "      <th>avg_engage_author</th>\n",
       "      <th>has_url</th>\n",
       "      <th>url_popularity</th>\n",
       "      <th>avg_engage_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1655713481254</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>595244</td>\n",
       "      <td>1654.123754</td>\n",
       "      <td>100</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1655743160254</td>\n",
       "      <td>100</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>595244</td>\n",
       "      <td>1654.123754</td>\n",
       "      <td>100</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1655395750254</td>\n",
       "      <td>100</td>\n",
       "      <td>-5</td>\n",
       "      <td>454592</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>595244</td>\n",
       "      <td>1654.123754</td>\n",
       "      <td>99</td>\n",
       "      <td>4025.303030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654621088254</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>19348</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1172</td>\n",
       "      <td>258.704778</td>\n",
       "      <td>99</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1655754531254</td>\n",
       "      <td>45</td>\n",
       "      <td>-5</td>\n",
       "      <td>973</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>595244</td>\n",
       "      <td>1654.123754</td>\n",
       "      <td>94</td>\n",
       "      <td>2.872340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp  feature1  feature2  followers  word_count  shared_url_count  \\\n",
       "0  1655713481254       100         0        999          25                 0   \n",
       "1  1655743160254       100        -5          5           9                 0   \n",
       "2  1655395750254       100        -5     454592          45                 0   \n",
       "3  1654621088254       100         5      19348          10                 0   \n",
       "4  1655754531254        45        -5        973          14                 0   \n",
       "\n",
       "   is_reply  is_retweet  contains_video  contains_image  language_popularity  \\\n",
       "0         0           1               1               0               595244   \n",
       "1         0           0               0               0               595244   \n",
       "2         0           0               0               0               595244   \n",
       "3         0           0               0               0                 1172   \n",
       "4         0           1               0               1               595244   \n",
       "\n",
       "   avg_engage_language  author_popularity  avg_engage_author  has_url  \\\n",
       "0          1654.123754                100           0.510000        0   \n",
       "1          1654.123754                100           0.020000        0   \n",
       "2          1654.123754                 99        4025.303030        0   \n",
       "3           258.704778                 99           4.454545        0   \n",
       "4          1654.123754                 94           2.872340        0   \n",
       "\n",
       "   url_popularity  avg_engage_url  \n",
       "0               0             0.0  \n",
       "1               0             0.0  \n",
       "2               0             0.0  \n",
       "3               0             0.0  \n",
       "4               0             0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "cAyvl08rEyZj",
    "outputId": "fe5f88bb-58d5-4347-f550-7474877825c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>contains_video</th>\n",
       "      <th>contains_image</th>\n",
       "      <th>language_popularity</th>\n",
       "      <th>avg_engage_language</th>\n",
       "      <th>author_popularity</th>\n",
       "      <th>avg_engage_author</th>\n",
       "      <th>has_url</th>\n",
       "      <th>url_popularity</th>\n",
       "      <th>avg_engage_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.354130e+05</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>6.354130e+05</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "      <td>635413.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.654867e+12</td>\n",
       "      <td>66.005028</td>\n",
       "      <td>-0.325403</td>\n",
       "      <td>2.604219e+05</td>\n",
       "      <td>17.832822</td>\n",
       "      <td>0.114296</td>\n",
       "      <td>0.199469</td>\n",
       "      <td>0.127155</td>\n",
       "      <td>0.138028</td>\n",
       "      <td>0.189636</td>\n",
       "      <td>557820.167178</td>\n",
       "      <td>1617.903398</td>\n",
       "      <td>92.140109</td>\n",
       "      <td>1617.903398</td>\n",
       "      <td>0.078938</td>\n",
       "      <td>31.709606</td>\n",
       "      <td>85.825317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.300051e+08</td>\n",
       "      <td>38.953551</td>\n",
       "      <td>3.454980</td>\n",
       "      <td>2.303286e+06</td>\n",
       "      <td>13.698307</td>\n",
       "      <td>0.423410</td>\n",
       "      <td>0.399601</td>\n",
       "      <td>0.333147</td>\n",
       "      <td>0.344930</td>\n",
       "      <td>0.392013</td>\n",
       "      <td>144063.300916</td>\n",
       "      <td>327.065817</td>\n",
       "      <td>17.685680</td>\n",
       "      <td>7903.916547</td>\n",
       "      <td>0.269642</td>\n",
       "      <td>226.993412</td>\n",
       "      <td>1571.884062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.653286e+12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.654277e+12</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>4.450000e+02</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>595244.000000</td>\n",
       "      <td>1654.123754</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>1.783333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.655001e+12</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.064000e+03</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>595244.000000</td>\n",
       "      <td>1654.123754</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>37.179775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.655534e+12</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.881600e+04</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>595244.000000</td>\n",
       "      <td>1654.123754</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>526.787879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.655790e+12</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.087310e+08</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>595244.000000</td>\n",
       "      <td>16805.666667</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>443919.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3037.000000</td>\n",
       "      <td>109872.485149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp       feature1       feature2     followers  \\\n",
       "count  6.354130e+05  635413.000000  635413.000000  6.354130e+05   \n",
       "mean   1.654867e+12      66.005028      -0.325403  2.604219e+05   \n",
       "std    7.300051e+08      38.953551       3.454980  2.303286e+06   \n",
       "min    1.653286e+12       0.000000      -5.000000  0.000000e+00   \n",
       "25%    1.654277e+12      29.000000      -5.000000  4.450000e+02   \n",
       "50%    1.655001e+12      88.000000       0.000000  2.064000e+03   \n",
       "75%    1.655534e+12     100.000000       0.000000  1.881600e+04   \n",
       "max    1.655790e+12     100.000000       5.000000  1.087310e+08   \n",
       "\n",
       "          word_count  shared_url_count       is_reply     is_retweet  \\\n",
       "count  635413.000000     635413.000000  635413.000000  635413.000000   \n",
       "mean       17.832822          0.114296       0.199469       0.127155   \n",
       "std        13.698307          0.423410       0.399601       0.333147   \n",
       "min         0.000000          0.000000       0.000000       0.000000   \n",
       "25%         8.000000          0.000000       0.000000       0.000000   \n",
       "50%        14.000000          0.000000       0.000000       0.000000   \n",
       "75%        25.000000          0.000000       0.000000       0.000000   \n",
       "max       129.000000          4.000000       1.000000       1.000000   \n",
       "\n",
       "       contains_video  contains_image  language_popularity  \\\n",
       "count   635413.000000   635413.000000        635413.000000   \n",
       "mean         0.138028        0.189636        557820.167178   \n",
       "std          0.344930        0.392013        144063.300916   \n",
       "min          0.000000        0.000000             1.000000   \n",
       "25%          0.000000        0.000000        595244.000000   \n",
       "50%          0.000000        0.000000        595244.000000   \n",
       "75%          0.000000        0.000000        595244.000000   \n",
       "max          1.000000        1.000000        595244.000000   \n",
       "\n",
       "       avg_engage_language  author_popularity  avg_engage_author  \\\n",
       "count        635413.000000      635413.000000      635413.000000   \n",
       "mean           1617.903398          92.140109        1617.903398   \n",
       "std             327.065817          17.685680        7903.916547   \n",
       "min               0.000000           1.000000           0.000000   \n",
       "25%            1654.123754          96.000000           1.783333   \n",
       "50%            1654.123754          99.000000          37.179775   \n",
       "75%            1654.123754         100.000000         526.787879   \n",
       "max           16805.666667         107.000000      443919.000000   \n",
       "\n",
       "             has_url  url_popularity  avg_engage_url  \n",
       "count  635413.000000   635413.000000   635413.000000  \n",
       "mean        0.078938       31.709606       85.825317  \n",
       "std         0.269642      226.993412     1571.884062  \n",
       "min         0.000000        0.000000        0.000000  \n",
       "25%         0.000000        0.000000        0.000000  \n",
       "50%         0.000000        0.000000        0.000000  \n",
       "75%         0.000000        0.000000        0.000000  \n",
       "max         1.000000     3037.000000   109872.485149  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M8o5lDMcEyZk"
   },
   "outputs": [],
   "source": [
    "std_cols = [\"timestamp\",\"feature1\",\"feature2\",\"followers\",\"word_count\"]\n",
    "std_scaler = preprocessing.MinMaxScaler().fit(X[std_cols])\n",
    "X1 = std_scaler.transform(X[std_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5nArdgUWEyZm"
   },
   "outputs": [],
   "source": [
    "sparse_cols = [\"shared_url_count\", \"language_popularity\", \"avg_engage_language\",\"author_popularity\",\n",
    "               \"avg_engage_author\",\"url_popularity\", \"avg_engage_url\"]\n",
    "sparse_scaler = preprocessing.MaxAbsScaler().fit(X[sparse_cols])\n",
    "X2 = sparse_scaler.transform(X[sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OFVsUWqKEyZn"
   },
   "outputs": [],
   "source": [
    "X[std_cols] = X1\n",
    "X[sparse_cols] = X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.drop(labels=[\"timestamp\",],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.drop(labels=[\"has_url\",],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scale_cols =  [\"feature1\",\"feature2\",\"followers\",\"word_count\",\"shared_url_count\", \"language_popularity\", \n",
    "         \"avg_engage_language\",\"author_popularity\", \"avg_engage_author\",\"url_popularity\", \"avg_engage_url\"]\n",
    "\n",
    "minmax_scaler = preprocessing.MinMaxScaler().fit(X[scale_cols])\n",
    "X[scale_cols] = minmax_scaler.transform(X[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cols =  [\"feature1\",\"feature2\",\"followers\",\"word_count\",\"shared_url_count\", \"language_popularity\", \n",
    "         \"avg_engage_language\",\"author_popularity\", \"avg_engage_author\",\"url_popularity\", \"avg_engage_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORizGXUJEyZo",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>contains_video</th>\n",
       "      <th>contains_image</th>\n",
       "      <th>language_popularity</th>\n",
       "      <th>avg_engage_language</th>\n",
       "      <th>author_popularity</th>\n",
       "      <th>avg_engage_author</th>\n",
       "      <th>url_popularity</th>\n",
       "      <th>avg_engage_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.187809e-06</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>1.148858e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.598503e-08</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>4.505326e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.180885e-03</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>9.067652e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.779437e-04</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.003459e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.948687e-06</td>\n",
       "      <td>0.108527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>6.470416e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2     followers  word_count  shared_url_count  is_reply  \\\n",
       "0      1.00       0.5  9.187809e-06    0.193798               0.0         0   \n",
       "1      1.00       0.0  4.598503e-08    0.069767               0.0         0   \n",
       "2      1.00       0.0  4.180885e-03    0.348837               0.0         0   \n",
       "3      1.00       1.0  1.779437e-04    0.077519               0.0         0   \n",
       "4      0.45       0.0  8.948687e-06    0.108527               0.0         0   \n",
       "\n",
       "   is_retweet  contains_video  contains_image  language_popularity  \\\n",
       "0           1               1               0             1.000000   \n",
       "1           0               0               0             1.000000   \n",
       "2           0               0               0             1.000000   \n",
       "3           0               0               0             0.001967   \n",
       "4           1               0               1             1.000000   \n",
       "\n",
       "   avg_engage_language  author_popularity  avg_engage_author  url_popularity  \\\n",
       "0             0.098427           0.933962       1.148858e-06             0.0   \n",
       "1             0.098427           0.933962       4.505326e-08             0.0   \n",
       "2             0.098427           0.924528       9.067652e-03             0.0   \n",
       "3             0.015394           0.924528       1.003459e-05             0.0   \n",
       "4             0.098427           0.877358       6.470416e-06             0.0   \n",
       "\n",
       "   avg_engage_url  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "QJU3M5ubEyZp",
    "outputId": "0728c020-97c6-4929-a174-d4b541ed2385"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['engagement', 'timestamp', 'language', 'feature1', 'feature2',\n",
       "       'followers', 'author', 'word_count', 'shared_url_count',\n",
       "       'shared_url_domain', 'is_reply', 'is_retweet', 'contains_video',\n",
       "       'contains_image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SHxINbwREyZq",
    "outputId": "7c3e427f-1c35-48fa-93e1-6823d59f0054"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:4034: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:3391: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/FanJiang/anaconda/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "Data_test = D_test[cols.delete(0)]\n",
    "Data_test.fillna(\"\",inplace=True)\n",
    "Data_test[\"language_popularity\"] = Data_test.apply(lambda row: dict_language.get(row[\"language\"],global_lang_avg_pop),axis = 1)\n",
    "Data_test[\"avg_engage_language\"] = Data_test.apply(lambda row: average_engage.get(row[\"language\"],global_avg_engage_lang),axis = 1)\n",
    "Data_test[\"author_popularity\"] = Data_test.apply(lambda row: dict_author.get(row[\"author\"],global_author_avg_pop),axis = 1)\n",
    "Data_test[\"avg_engage_author\"] = Data_test.apply(lambda row: avg_engage_author.get(row[\"author\"],global_avg_engage_author),axis = 1)\n",
    "Data_test[\"has_url\"] = Data_test.apply(lambda row: 0 if row[\"shared_url_domain\"] == \"\" else 1,axis = 1)\n",
    "Data_test[\"url_popularity\"] = Data_test.apply(lambda row: dict_url.get(row[\"shared_url_domain\"],0) if row[\"has_url\"] else 0,axis = 1)\n",
    "Data_test[\"avg_engage_url\"] = Data_test.apply(lambda row: avg_engage_url.get(row[\"shared_url_domain\"],0) if row[\"has_url\"] else 0,axis = 1)\n",
    "\n",
    "boolean_cols = [\"is_reply\", \"is_retweet\", \"contains_video\", \"contains_image\"]\n",
    "Data_test[boolean_cols] = Data_test[boolean_cols].astype(int)\n",
    "dropped_cols = [\"language\", \"author\", \"shared_url_domain\"]\n",
    "Data_test.drop(labels=dropped_cols, axis=1, inplace=True)\n",
    "\"\"\"\n",
    "Data_test1 = std_scaler.transform(Data_test[std_cols])\n",
    "Data_test2 = sparse_scaler.transform(Data_test[sparse_cols])\n",
    "Data_test[std_cols] = Data_test1\n",
    "Data_test[sparse_cols] = Data_test2\n",
    "\"\"\"\n",
    "Data_test[scale_cols] = minmax_scaler.transform(Data_test[scale_cols])\n",
    "\n",
    "Data_test.drop(labels=[\"timestamp\",],axis = 1, inplace=True)\n",
    "Data_test.drop(labels=[\"has_url\",],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "fr55zoxBOo6J",
    "outputId": "639a72e9-e58c-48ea-fb74-a57057315d6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>contains_video</th>\n",
       "      <th>contains_image</th>\n",
       "      <th>language_popularity</th>\n",
       "      <th>avg_engage_language</th>\n",
       "      <th>author_popularity</th>\n",
       "      <th>avg_engage_author</th>\n",
       "      <th>url_popularity</th>\n",
       "      <th>avg_engage_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.275973e-05</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>8.025984e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.251621e-04</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.849072e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.589582e-04</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>8.611613e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.696286e-04</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.718031e-03</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>2.383672e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.416339e-06</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>1.446064e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.563491e-07</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>1.486758e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.501586e-05</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.995541e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.336039e-05</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>1.100321e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.042433e-06</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>2.915211e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.060323e-03</td>\n",
       "      <td>0.085271</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>5.148204e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.520677e-06</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>1.648857e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.248953e-05</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>2.084510e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.595037e-04</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>9.684391e-02</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>2.230638e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.351960e-06</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.537736</td>\n",
       "      <td>1.689497e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.186414e-05</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>4.763031e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.187968e-05</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>2.183035e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.196942e-05</td>\n",
       "      <td>0.217054</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>4.321018e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.167468e-03</td>\n",
       "      <td>0.201550</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>3.094952e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.110059e-04</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>9.056892e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.264937e-06</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>4.663408e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.839401e-08</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.747431e-07</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.774826e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.197006e-07</td>\n",
       "      <td>0.062016</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.726173</td>\n",
       "      <td>6.290708e-03</td>\n",
       "      <td>0.032927</td>\n",
       "      <td>1.820292e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.325700e-05</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>1.581370e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.401116e-06</td>\n",
       "      <td>0.178295</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.462264</td>\n",
       "      <td>1.255184e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.154065e-05</td>\n",
       "      <td>0.379845</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>1.713725e-03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.493532e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.147850e-06</td>\n",
       "      <td>0.062016</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.365250e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.256760e-03</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.764151</td>\n",
       "      <td>2.082854e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.231542e-05</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>1.310725e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.536026e-05</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>5.095342e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.327128e-05</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.524530e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.251490e-06</td>\n",
       "      <td>0.155039</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>1.077361e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.212965e-03</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>3.013174e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.182919e-04</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>3.460042e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.252347e-05</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>8.132342e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.397945e-05</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>9.157076e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.121648e-04</td>\n",
       "      <td>0.178295</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>2.467714e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.074725e-03</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>2.540889e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.238900e-05</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.010513e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.874037e-03</td>\n",
       "      <td>0.240310</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>2.334044e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.426233e-07</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>4.596343e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>0.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.283166e-04</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>5.389781e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.493532e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.238043e-06</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>1.788197e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>0.56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.100088e-06</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>1.058752e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.172967e-06</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>0.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105480e-05</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>8.309597e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.577921e-05</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>1.569897e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.384397e-03</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>5.500822e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.207281e-07</td>\n",
       "      <td>0.085271</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>5.005918e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.168020e-05</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.922728e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.172967e-06</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>6.082191e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.076050e-05</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.556604</td>\n",
       "      <td>1.255484e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.886819e-04</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>2.245204e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.541545e-06</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.764049e-06</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.956859e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.369434e-05</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>5.726270e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.127553e-05</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>3.050481e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.451655e-04</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.306317e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.115311e-07</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>1.123448e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1  feature2     followers  word_count  shared_url_count  \\\n",
       "0         1.00       0.0  3.275973e-05    0.046512              0.00   \n",
       "1         0.13       0.0  1.251621e-04    0.054264              0.00   \n",
       "2         1.00       0.5  4.589582e-04    0.093023              0.00   \n",
       "3         0.95       0.0  2.696286e-04    0.170543              0.25   \n",
       "4         1.00       0.5  1.416339e-06    0.023256              0.00   \n",
       "5         1.00       0.0  1.563491e-07    0.054264              0.00   \n",
       "6         1.00       0.5  2.501586e-05    0.186047              0.00   \n",
       "7         1.00       0.0  2.336039e-05    0.093023              0.00   \n",
       "8         0.17       0.5  6.042433e-06    0.046512              0.00   \n",
       "9         0.38       0.0  1.060323e-03    0.085271              0.00   \n",
       "10        1.00       0.0  6.520677e-06    0.100775              0.00   \n",
       "11        0.38       0.5  1.248953e-05    0.255814              0.00   \n",
       "12        0.58       0.0  1.595037e-04    0.100775              0.25   \n",
       "13        1.00       0.0  1.351960e-06    0.209302              0.00   \n",
       "14        0.00       0.5  1.186414e-05    0.116279              0.00   \n",
       "15        0.41       0.5  2.187968e-05    0.333333              0.00   \n",
       "16        0.34       0.5  6.196942e-05    0.217054              0.00   \n",
       "17        0.87       0.5  1.167468e-03    0.201550              0.00   \n",
       "18        0.50       1.0  3.110059e-04    0.093023              0.00   \n",
       "19        1.00       0.5  3.264937e-06    0.147287              0.00   \n",
       "20        0.50       1.0  1.839401e-08    0.100775              0.00   \n",
       "21        1.00       0.0  1.747431e-07    0.356589              0.00   \n",
       "22        0.29       0.5  9.197006e-07    0.062016              0.25   \n",
       "23        1.00       1.0  6.325700e-05    0.162791              0.00   \n",
       "24        0.39       0.5  6.401116e-06    0.178295              0.00   \n",
       "25        0.77       0.5  8.154065e-05    0.379845              0.25   \n",
       "26        1.00       0.5  4.147850e-06    0.062016              0.00   \n",
       "27        1.00       0.5  4.256760e-03    0.131783              0.00   \n",
       "28        1.00       0.5  4.231542e-05    0.093023              0.00   \n",
       "29        1.00       0.0  7.536026e-05    0.077519              0.00   \n",
       "...        ...       ...           ...         ...               ...   \n",
       "2470      1.00       0.5  1.327128e-05    0.023256              0.00   \n",
       "2471      0.73       0.5  5.251490e-06    0.155039              0.00   \n",
       "2472      0.44       0.5  3.212965e-03    0.093023              0.00   \n",
       "2473      0.50       0.5  1.182919e-04    0.054264              0.00   \n",
       "2474      1.00       1.0  2.252347e-05    0.193798              0.00   \n",
       "2475      0.82       0.5  1.397945e-05    0.209302              0.00   \n",
       "2476      1.00       0.0  3.121648e-04    0.178295              0.00   \n",
       "2477      0.00       0.5  1.074725e-03    0.147287              0.00   \n",
       "2478      0.50       1.0  4.238900e-05    0.038760              0.00   \n",
       "2479      1.00       0.5  2.874037e-03    0.240310              0.00   \n",
       "2480      1.00       0.5  5.426233e-07    0.372093              0.00   \n",
       "2481      0.09       1.0  1.283166e-04    0.279070              1.25   \n",
       "2482      1.00       0.0  7.238043e-06    0.131783              0.00   \n",
       "2483      0.56       1.0  7.100088e-06    0.116279              0.50   \n",
       "2484      1.00       0.0  3.172967e-06    0.077519              0.00   \n",
       "2485      0.69       1.0  1.105480e-05    0.124031              0.00   \n",
       "2486      1.00       0.5  2.577921e-05    0.116279              0.00   \n",
       "2487      1.00       1.0  4.384397e-03    0.286822              0.00   \n",
       "2488      0.25       1.0  2.207281e-07    0.085271              0.00   \n",
       "2489      0.25       0.5  1.168020e-05    0.038760              0.00   \n",
       "2490      1.00       0.5  3.172967e-06    0.038760              0.00   \n",
       "2491      1.00       0.5  1.076050e-05    0.046512              0.00   \n",
       "2492      1.00       1.0  5.886819e-04    0.255814              0.00   \n",
       "2493      1.00       1.0  7.541545e-06    0.031008              0.00   \n",
       "2494      1.00       0.0  4.764049e-06    0.007752              0.00   \n",
       "2495      1.00       0.0  1.369434e-05    0.007752              0.00   \n",
       "2496      0.67       1.0  1.127553e-05    0.031008              0.00   \n",
       "2497      0.20       0.0  1.451655e-04    0.031008              0.00   \n",
       "2498      1.00       0.0  0.000000e+00    0.054264              0.00   \n",
       "2499      1.00       1.0  2.115311e-07    0.131783              0.00   \n",
       "\n",
       "      is_reply  is_retweet  contains_video  contains_image  \\\n",
       "0            0           0               0               0   \n",
       "1            0           0               1               0   \n",
       "2            0           0               0               0   \n",
       "3            0           0               0               0   \n",
       "4            0           0               0               0   \n",
       "5            0           0               0               0   \n",
       "6            0           0               0               0   \n",
       "7            0           0               0               1   \n",
       "8            0           0               0               0   \n",
       "9            1           0               0               1   \n",
       "10           0           0               0               0   \n",
       "11           0           0               0               1   \n",
       "12           1           0               0               0   \n",
       "13           1           0               0               0   \n",
       "14           0           0               0               1   \n",
       "15           0           0               0               1   \n",
       "16           0           0               0               1   \n",
       "17           0           0               0               0   \n",
       "18           0           0               0               1   \n",
       "19           0           0               0               0   \n",
       "20           0           0               0               0   \n",
       "21           1           0               0               0   \n",
       "22           0           0               0               0   \n",
       "23           0           0               1               0   \n",
       "24           0           0               0               1   \n",
       "25           0           0               0               0   \n",
       "26           0           0               0               0   \n",
       "27           0           0               1               0   \n",
       "28           0           0               0               0   \n",
       "29           0           0               0               0   \n",
       "...        ...         ...             ...             ...   \n",
       "2470         0           0               0               0   \n",
       "2471         0           0               0               0   \n",
       "2472         0           0               1               0   \n",
       "2473         0           0               0               1   \n",
       "2474         0           0               1               0   \n",
       "2475         0           0               0               0   \n",
       "2476         0           0               0               0   \n",
       "2477         0           0               0               1   \n",
       "2478         0           0               0               0   \n",
       "2479         0           0               1               0   \n",
       "2480         1           0               0               0   \n",
       "2481         0           0               0               1   \n",
       "2482         1           0               0               0   \n",
       "2483         0           0               0               1   \n",
       "2484         0           0               0               0   \n",
       "2485         0           0               0               1   \n",
       "2486         1           0               0               0   \n",
       "2487         0           0               1               0   \n",
       "2488         0           0               0               1   \n",
       "2489         1           0               0               0   \n",
       "2490         0           0               0               0   \n",
       "2491         0           0               0               0   \n",
       "2492         0           0               0               1   \n",
       "2493         0           0               0               0   \n",
       "2494         0           0               0               0   \n",
       "2495         0           0               0               0   \n",
       "2496         0           0               0               0   \n",
       "2497         0           0               1               0   \n",
       "2498         0           0               0               0   \n",
       "2499         0           0               0               1   \n",
       "\n",
       "      language_popularity  avg_engage_language  author_popularity  \\\n",
       "0                     1.0             0.098427           0.905660   \n",
       "1                     1.0             0.098427           0.924528   \n",
       "2                     1.0             0.098427           0.858491   \n",
       "3                     1.0             0.098427           0.924528   \n",
       "4                     1.0             0.098427           0.867925   \n",
       "5                     1.0             0.098427           0.933962   \n",
       "6                     1.0             0.098427           0.924528   \n",
       "7                     1.0             0.098427           0.905660   \n",
       "8                     1.0             0.098427           0.792453   \n",
       "9                     1.0             0.098427           0.896226   \n",
       "10                    1.0             0.098427           0.905660   \n",
       "11                    1.0             0.098427           0.924528   \n",
       "12                    1.0             0.098427           0.933962   \n",
       "13                    1.0             0.098427           0.537736   \n",
       "14                    1.0             0.098427           0.037736   \n",
       "15                    1.0             0.098427           0.924528   \n",
       "16                    1.0             0.098427           0.924528   \n",
       "17                    1.0             0.098427           0.915094   \n",
       "18                    1.0             0.098427           0.169811   \n",
       "19                    1.0             0.098427           0.528302   \n",
       "20                    1.0             0.098427           0.943396   \n",
       "21                    1.0             0.098427           0.924528   \n",
       "22                    1.0             0.098427           0.726173   \n",
       "23                    1.0             0.098427           0.509434   \n",
       "24                    1.0             0.098427           0.462264   \n",
       "25                    1.0             0.098427           0.915094   \n",
       "26                    1.0             0.098427           0.924528   \n",
       "27                    1.0             0.098427           0.764151   \n",
       "28                    1.0             0.098427           0.905660   \n",
       "29                    1.0             0.098427           0.924528   \n",
       "...                   ...                  ...                ...   \n",
       "2470                  1.0             0.098427           0.924528   \n",
       "2471                  1.0             0.098427           0.207547   \n",
       "2472                  1.0             0.098427           0.924528   \n",
       "2473                  1.0             0.098427           0.858491   \n",
       "2474                  1.0             0.098427           0.924528   \n",
       "2475                  1.0             0.098427           0.933962   \n",
       "2476                  1.0             0.098427           0.943396   \n",
       "2477                  1.0             0.098427           0.915094   \n",
       "2478                  1.0             0.098427           0.924528   \n",
       "2479                  1.0             0.098427           0.915094   \n",
       "2480                  1.0             0.098427           0.924528   \n",
       "2481                  1.0             0.098427           0.924528   \n",
       "2482                  1.0             0.098427           0.905660   \n",
       "2483                  1.0             0.098427           0.933962   \n",
       "2484                  1.0             0.098427           0.924528   \n",
       "2485                  1.0             0.098427           0.924528   \n",
       "2486                  1.0             0.098427           0.905660   \n",
       "2487                  1.0             0.098427           0.924528   \n",
       "2488                  1.0             0.098427           0.415094   \n",
       "2489                  1.0             0.098427           0.924528   \n",
       "2490                  1.0             0.098427           0.933962   \n",
       "2491                  1.0             0.098427           0.556604   \n",
       "2492                  1.0             0.098427           0.415094   \n",
       "2493                  1.0             0.098427           0.386792   \n",
       "2494                  1.0             0.098427           0.924528   \n",
       "2495                  1.0             0.098427           0.037736   \n",
       "2496                  1.0             0.098427           0.896226   \n",
       "2497                  1.0             0.098427           0.924528   \n",
       "2498                  1.0             0.098427           0.924528   \n",
       "2499                  1.0             0.098427           0.226415   \n",
       "\n",
       "      avg_engage_author  url_popularity  avg_engage_url  \n",
       "0          8.025984e-05        0.000000    0.000000e+00  \n",
       "1          1.849072e-03        0.000000    0.000000e+00  \n",
       "2          8.611613e-03        0.000000    0.000000e+00  \n",
       "3          1.718031e-03        0.000329    2.383672e-02  \n",
       "4          1.446064e-05        0.000000    0.000000e+00  \n",
       "5          1.486758e-06        0.000000    0.000000e+00  \n",
       "6          1.995541e-05        0.000000    0.000000e+00  \n",
       "7          1.100321e-04        0.000000    0.000000e+00  \n",
       "8          2.915211e-07        0.000000    0.000000e+00  \n",
       "9          5.148204e-03        0.000000    0.000000e+00  \n",
       "10         1.648857e-06        0.000000    0.000000e+00  \n",
       "11         2.084510e-03        0.000000    0.000000e+00  \n",
       "12         9.684391e-02        0.002305    2.230638e-01  \n",
       "13         1.689497e-05        0.000000    0.000000e+00  \n",
       "14         4.763031e-03        0.000000    0.000000e+00  \n",
       "15         2.183035e-04        0.000000    0.000000e+00  \n",
       "16         4.321018e-05        0.000000    0.000000e+00  \n",
       "17         3.094952e-03        0.000000    0.000000e+00  \n",
       "18         9.056892e-04        0.000000    0.000000e+00  \n",
       "19         4.663408e-06        0.000000    0.000000e+00  \n",
       "20         0.000000e+00        0.000000    0.000000e+00  \n",
       "21         1.774826e-06        0.000000    0.000000e+00  \n",
       "22         6.290708e-03        0.032927    1.820292e-07  \n",
       "23         1.581370e-04        0.000000    0.000000e+00  \n",
       "24         1.255184e-04        0.000000    0.000000e+00  \n",
       "25         1.713725e-03        1.000000    1.493532e-02  \n",
       "26         1.365250e-06        0.000000    0.000000e+00  \n",
       "27         2.082854e-01        0.000000    0.000000e+00  \n",
       "28         1.310725e-04        0.000000    0.000000e+00  \n",
       "29         5.095342e-04        0.000000    0.000000e+00  \n",
       "...                 ...             ...             ...  \n",
       "2470       1.524530e-05        0.000000    0.000000e+00  \n",
       "2471       1.077361e-06        0.000000    0.000000e+00  \n",
       "2472       3.013174e-02        0.000000    0.000000e+00  \n",
       "2473       3.460042e-04        0.000000    0.000000e+00  \n",
       "2474       8.132342e-04        0.000000    0.000000e+00  \n",
       "2475       9.157076e-05        0.000000    0.000000e+00  \n",
       "2476       2.467714e-03        0.000000    0.000000e+00  \n",
       "2477       2.540889e-03        0.000000    0.000000e+00  \n",
       "2478       1.010513e-04        0.000000    0.000000e+00  \n",
       "2479       2.334044e-02        0.000000    0.000000e+00  \n",
       "2480       4.596343e-06        0.000000    0.000000e+00  \n",
       "2481       5.389781e-04        1.000000    1.493532e-02  \n",
       "2482       1.788197e-05        0.000000    0.000000e+00  \n",
       "2483       1.058752e-05        0.000000    0.000000e+00  \n",
       "2484       0.000000e+00        0.000000    0.000000e+00  \n",
       "2485       8.309597e-04        0.000000    0.000000e+00  \n",
       "2486       1.569897e-05        0.000000    0.000000e+00  \n",
       "2487       5.500822e-04        0.000000    0.000000e+00  \n",
       "2488       5.005918e-08        0.000000    0.000000e+00  \n",
       "2489       1.922728e-05        0.000000    0.000000e+00  \n",
       "2490       6.082191e-07        0.000000    0.000000e+00  \n",
       "2491       1.255484e-04        0.000000    0.000000e+00  \n",
       "2492       2.245204e-03        0.000000    0.000000e+00  \n",
       "2493       0.000000e+00        0.000000    0.000000e+00  \n",
       "2494       1.956859e-05        0.000000    0.000000e+00  \n",
       "2495       5.726270e-04        0.000000    0.000000e+00  \n",
       "2496       3.050481e-07        0.000000    0.000000e+00  \n",
       "2497       1.306317e-04        0.000000    0.000000e+00  \n",
       "2498       0.000000e+00        0.000000    0.000000e+00  \n",
       "2499       1.123448e-03        0.000000    0.000000e+00  \n",
       "\n",
       "[2500 rows x 15 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1mVJMM4_EyZs",
    "outputId": "932b9135-b0c0-4f26-8661-22bd69886aa5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFR = RandomForestRegressor(n_estimators = 100, criterion='mse', verbose= True, ).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hBjj2uFYEyZv",
    "outputId": "f0095d18-7c59-48fa-bc19-15ff21ee90c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   10.2s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = RFR.predict(Data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "vrCRSLcWUDco",
    "outputId": "da5d94c4-b5cf-4bed-c047-752c2aa6c50b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   7.16610714,   94.99      , 3821.776     , ...,  137.82      ,\n",
       "          0.        , 6294.95      ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb = xgboost.XGBRegressor(max_depth=7, n_estimators=200)\n",
    "xgb.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(Data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-13.81551056, -13.81551056,   8.15651023, ...,  10.37311611,\n",
       "       -13.81551056,   9.61159664])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform \n",
    "import xgboost\n",
    "xgb = xgboost.XGBRegressor(n_estimators= 400, max_depth= 9, n_jobs = 4)\n",
    "xgb.fit(X,np.log(Y+1e-6))\n",
    "y_pred = xgb.predict(Data_test)\n",
    "y_pred = np.exp(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(Data_test)\n",
    "y_pred = np.exp(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3906806e+00, 4.3475571e+02, 1.1323616e+03, ..., 2.8203634e+01,\n",
       "       1.0065970e-06, 8.9912605e-01], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "90Us-WVDEyZw"
   },
   "outputs": [],
   "source": [
    "D_pred = pd.DataFrame(y_pred, columns=[\"engagement\"])\n",
    "D_pred.index +=1\n",
    "D_pred.to_csv(\"submission_XGB_400_9_log.csv\",index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FMPpdTpmEyZz"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3JwEOs8ZEyZ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EqEAvVuxEyZ1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9U2dC6wVEyZ2"
   },
   "outputs": [],
   "source": [
    "new_X = X.drop(labels=[\"language_popularity\",\"author_popularity\", \"url_popularity\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPuvleOLEyZ3"
   },
   "outputs": [],
   "source": [
    "num = [100,200,400]\n",
    "depth = [5,7,9]\n",
    "\n",
    "for n in num:\n",
    "    for d in depth:\n",
    "        xgb = xgboost.XGBRegressor(n_estimators = n, max_depth = d, n_jobs = 4)\n",
    "        pred = cross_val_predict(xgb, X, np.log(Y + 1e-6), cv=5)\n",
    "        print(n,d, MAE(np.exp(pred),Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5 6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_2qqCOTEyZ4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-13.759142 , -13.537834 ,   7.9405985, ...,   7.7445745,\n",
       "       -12.337345 ,   9.465805 ], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103.9909854553175"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(np.exp(pred), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-q5ABfyhEyZ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1075.5897296714982"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb = xgboost.XGBRegressor(max_depth=7, n_estimators=200,n_jobs = 4)\n",
    "xgb.fit(X_train,np.log(y_train+1e-6))\n",
    "y_pred = xgb.predict(X_test)\n",
    "MAE(y_test, np.exp(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RN0wAaCWEyZ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152.4350997759527"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = xgboost.XGBRegressor(n_jobs = 4)\n",
    "xgb.fit(X_train,np.log(y_train+1e-6))\n",
    "y_pred = xgb.predict(X_test)\n",
    "MAE(y_test, np.exp(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rt31yIrzEyZ7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "egY06-1oEyZ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0fcLDf6uEyZ9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Bo9eh9taEyZ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjgCiCGpEyZ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 5 -1303.1202284474662\n",
      "100 7 -1267.4712181904038\n",
      "100 9 -1254.4365199906242\n",
      "200 5 -1302.695449787905\n",
      "200 7 -1267.7194750046933\n",
      "200 9 -1252.6478957556403\n",
      "400 5 -1302.9217695187226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-887c0a6787c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mRFR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRFR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "num = [100,200,400]\n",
    "depth = [5,7,9]\n",
    "\n",
    "for n in num:\n",
    "    for d in depth:\n",
    "        RFR = RandomForestRegressor(n_estimators = n, max_depth = d, )\n",
    "        l = cross_val_score(RFR, X, Y, scoring='neg_mean_absolute_error', cv=5)\n",
    "        m =l.mean()\n",
    "        print(n, d, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "num = [100,200,400]\n",
    "depth = [5,7,9]\n",
    "new_X = X.drop(labels=[\"language_popularity\",\"author_popularity\", \"url_popularity\"],axis = 1)\n",
    "\n",
    "for n in num:\n",
    "    for d in depth:\n",
    "        RFR = RandomForestRegressor(n_estimators = n, max_depth = d, )\n",
    "        l = cross_val_score(RFR, new_X, Y, scoring='neg_mean_absolute_error', cv=5)\n",
    "        m =l.mean()\n",
    "        print(n, d, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0_cwWT8EyaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3 -1318.7712382356121\n",
      "100 5 -1275.1124933832325\n",
      "100 7 -1262.1717964424795\n",
      "200 3 -1320.7073751404391\n",
      "200 5 -1278.2913930617601\n",
      "200 7 -1266.5077456096994\n",
      "400 3 -1316.2311289689044\n",
      "400 5 -1282.7816364289386\n",
      "400 7 -1273.8679861007133\n"
     ]
    }
   ],
   "source": [
    "num = [100,200,400]\n",
    "depth = [3,5,7]\n",
    "\n",
    "for n in num:\n",
    "    for d in depth:\n",
    "        xgb = xgboost.XGBRegressor(max_depth=d, n_estimators=n)\n",
    "        l = cross_val_score(xgb, X, Y, scoring='neg_mean_absolute_error', cv=5)\n",
    "        m =l.mean()\n",
    "        print(n, d, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBRegressor()\n",
    "l = cross_val_score(xgb, X, Y, scoring='neg_mean_absolute_error', cv=5)\n",
    "m =l.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1318.7712382356121\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JXciv1drEyaC"
   },
   "outputs": [],
   "source": [
    "X.drop(labels=[\"has_url\",],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "id": "Rm2VwThDEyaE",
    "outputId": "5875035f-594b-44c8-c44e-786b845099f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>followers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shared_url_count</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>contains_video</th>\n",
       "      <th>contains_image</th>\n",
       "      <th>language_popularity</th>\n",
       "      <th>avg_engage_language</th>\n",
       "      <th>author_popularity</th>\n",
       "      <th>avg_engage_author</th>\n",
       "      <th>url_popularity</th>\n",
       "      <th>avg_engage_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.187809e-06</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>1.148858e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.598503e-08</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>4.505326e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.180885e-03</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>9.067652e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.779437e-04</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.003459e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.948687e-06</td>\n",
       "      <td>0.108527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>6.470416e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2     followers  word_count  shared_url_count  is_reply  \\\n",
       "0      1.00       0.5  9.187809e-06    0.193798               0.0         0   \n",
       "1      1.00       0.0  4.598503e-08    0.069767               0.0         0   \n",
       "2      1.00       0.0  4.180885e-03    0.348837               0.0         0   \n",
       "3      1.00       1.0  1.779437e-04    0.077519               0.0         0   \n",
       "4      0.45       0.0  8.948687e-06    0.108527               0.0         0   \n",
       "\n",
       "   is_retweet  contains_video  contains_image  language_popularity  \\\n",
       "0           1               1               0             1.000000   \n",
       "1           0               0               0             1.000000   \n",
       "2           0               0               0             1.000000   \n",
       "3           0               0               0             0.001967   \n",
       "4           1               0               1             1.000000   \n",
       "\n",
       "   avg_engage_language  author_popularity  avg_engage_author  url_popularity  \\\n",
       "0             0.098427           0.933962       1.148858e-06             0.0   \n",
       "1             0.098427           0.933962       4.505326e-08             0.0   \n",
       "2             0.098427           0.924528       9.067652e-03             0.0   \n",
       "3             0.015394           0.924528       1.003459e-05             0.0   \n",
       "4             0.098427           0.877358       6.470416e-06             0.0   \n",
       "\n",
       "   avg_engage_url  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XSkYWNfcEyaF"
   },
   "outputs": [],
   "source": [
    "X_values = X.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "uT8r-rplEyaG",
    "outputId": "de8f7829-b40c-45ad-85aa-14fde67a8c8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb = xgboost.XGBRegressor(max_depth=5, n_estimators=200)\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NlV0qE0qEyaH"
   },
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vP9XfoZ3EyaI",
    "outputId": "66788aba-5e70-4df7-b929-b34161323a63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4609951528723391"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bm60kD5_EyaJ",
    "outputId": "daff43b3-7698-4987-c01f-1db97aee16fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3944564606059692"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QkMapkeeEyaL",
    "outputId": "11615a5d-be2f-4ab0-924d-556442aae3ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260.966338753721"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qLNQVoYbEyaN",
    "outputId": "a755ca24-0500-44cf-abc1-50437310e475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  395.44727 ,    76.29369 ,  5589.2036  ,    76.29369 ,\n",
       "        5639.067   ,    16.873188,   -80.6459  ,    84.85982 ,\n",
       "          94.416595,   -80.6459  ,   -99.634895,    76.29369 ,\n",
       "         351.53516 ,   327.898   ,    27.77224 ,   156.53442 ,\n",
       "         257.1179  ,   -80.6459  , 14315.04    ,   583.6589  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "O7dx7x7QEyaR",
    "outputId": "07a51a32-5d8a-482c-a776-0d50bb4aa406"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  336,     4,  5509,     0,     8,     0,     0,     6,     0,\n",
       "           0,     0,    37,   304,     2,     1,     0,   117,     0,\n",
       "       16566,   123])"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iQaGdVWAEyaS",
    "outputId": "a69c9265-78dd-418a-a8d0-4133e1e45393"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  5.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFR = RandomForestRegressor(n_estimators = 100, verbose= True, ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LjwUTWuUEyaT",
    "outputId": "600ea51f-dedc-4d4c-a716-694a3e2ab163"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.9s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = RFR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Io90-DabEyaU",
    "outputId": "c0486230-78e2-49ae-a808-ff56a69942c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9089074535161289"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFR.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "o3Wd3B14EyaV",
    "outputId": "84547396-44ea-411a-8812-665170673e79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3640630957316382"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFR.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uD2VVanFEyaW",
    "outputId": "771205b0-611b-4a4d-d685-48f5ae3b5bc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1252.8560582150606"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "VqmLVW8JEyaX",
    "outputId": "90b68961-5eea-488f-9a5d-9f31c20e416d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.18840000e+02, 5.50000000e-01, 4.77804000e+03, 1.63000000e+00,\n",
       "       3.63176000e+03, 2.00000000e-02, 0.00000000e+00, 3.50550000e+01,\n",
       "       2.76666667e-01, 0.00000000e+00, 3.43333333e-01, 4.79800000e+01,\n",
       "       4.05320000e+02, 1.07754000e+03, 3.48000000e+00, 0.00000000e+00,\n",
       "       1.01794167e+02, 0.00000000e+00, 1.60238100e+04, 1.04341000e+03])"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "z-Mr2oejEyaY",
    "outputId": "2ccf0cd1-4263-4fc9-88fa-6c11e8f2f104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  336,     4,  5509,     0,     8,     0,     0,     6,     0,\n",
       "           0,     0,    37,   304,     2,     1,     0,   117,     0,\n",
       "       16566,   123])"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sy2RlkCtEyaZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eZOTV26KEyaa"
   },
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfhRJ64QEyab",
    "outputId": "3aa7a09f-0728-4b5f-e78a-e85c5e0c7dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1635.5831644076763"
      ]
     },
     "execution_count": 236,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "te15SMnvEyac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_Fg3CIGvEyai"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uT0SqEVgEyaj",
    "outputId": "12bc7444-aa18-4cb0-9b4c-28e8619f32f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3608150501866566"
      ]
     },
     "execution_count": 239,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUkZ2zxyEyal",
    "outputId": "2ca8cf07-2a23-457d-e9e2-2717aff97e2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3477166125104979"
      ]
     },
     "execution_count": 240,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "YGf2qEHFEyam",
    "outputId": "06efc05a-a4d8-4c7d-a50e-6dc7b573c8c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 70591370.59141135\n",
      "Iteration 2, loss = 53046783.15302167\n",
      "Iteration 3, loss = 52588482.49416848\n",
      "Iteration 4, loss = 52436854.65307368\n",
      "Iteration 5, loss = 52423860.41675160\n",
      "Iteration 6, loss = 52357652.34496766\n",
      "Iteration 7, loss = 52288476.89055861\n",
      "Iteration 8, loss = 52204840.55687391\n",
      "Iteration 9, loss = 52191867.08476962\n",
      "Iteration 10, loss = 52203043.48621771\n",
      "Iteration 11, loss = 52204566.45522118\n",
      "Iteration 12, loss = 52070223.61319903\n",
      "Iteration 13, loss = 52092306.45239532\n",
      "Iteration 14, loss = 52097597.01515065\n",
      "Iteration 15, loss = 51988251.08496352\n",
      "Iteration 16, loss = 51937624.24972596\n",
      "Iteration 17, loss = 52037636.13477058\n",
      "Iteration 18, loss = 51867758.76228215\n",
      "Iteration 19, loss = 51818939.33461963\n",
      "Iteration 20, loss = 51743153.48001513\n",
      "Iteration 21, loss = 51803380.46708936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "nn = MLPRegressor(hidden_layer_sizes=(256, 128, 64, 16), verbose=True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uf5Z9O2QEyao"
   },
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uz6yMiEaEyao",
    "outputId": "7d482afa-e1e1-4741-8f99-9a19d0f7658d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1640.119569220309"
      ]
     },
     "execution_count": 243,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H7IJ21bnf1VD",
    "outputId": "c9e0b1ea-e44c-4911-f897-f949992e6947"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635413, 15)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SCbbDhJXEyap"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "id": "4KA-TXwFe0oP",
    "outputId": "9a3f0da8-15b8-4571-d23c-93fe454f8d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 46,305\n",
      "Trainable params: 46,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = 15\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim = 15,activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(\n",
    "    loss=keras.losses.mean_absolute_error,\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xunz3O2EgpGq",
    "outputId": "2d761d14-e061-4181-a6a7-c4158209c2b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 508330 samples, validate on 127083 samples\n",
      "Epoch 1/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1193.2902 - acc: 0.3136 - val_loss: 1173.0563 - val_acc: 0.3071\n",
      "Epoch 2/200\n",
      "508330/508330 [==============================] - 20s 39us/step - loss: 1178.2068 - acc: 0.3139 - val_loss: 1156.8293 - val_acc: 0.3245\n",
      "Epoch 3/200\n",
      "508330/508330 [==============================] - 20s 39us/step - loss: 1165.8309 - acc: 0.3140 - val_loss: 1144.7819 - val_acc: 0.3242\n",
      "Epoch 4/200\n",
      "508330/508330 [==============================] - 20s 39us/step - loss: 1156.7747 - acc: 0.3146 - val_loss: 1146.7910 - val_acc: 0.2968\n",
      "Epoch 5/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1150.7602 - acc: 0.3150 - val_loss: 1129.6018 - val_acc: 0.3168\n",
      "Epoch 6/200\n",
      "508330/508330 [==============================] - 20s 39us/step - loss: 1146.3995 - acc: 0.3147 - val_loss: 1126.4040 - val_acc: 0.3159\n",
      "Epoch 7/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1142.8934 - acc: 0.3146 - val_loss: 1123.1468 - val_acc: 0.3233\n",
      "Epoch 8/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1138.8219 - acc: 0.3147 - val_loss: 1123.1844 - val_acc: 0.3258\n",
      "Epoch 9/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1135.4768 - acc: 0.3142 - val_loss: 1116.8943 - val_acc: 0.3143\n",
      "Epoch 10/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1132.9926 - acc: 0.3134 - val_loss: 1115.2511 - val_acc: 0.3221\n",
      "Epoch 11/200\n",
      "508330/508330 [==============================] - 21s 41us/step - loss: 1131.3037 - acc: 0.3135 - val_loss: 1114.3058 - val_acc: 0.3228\n",
      "Epoch 12/200\n",
      "508330/508330 [==============================] - 21s 41us/step - loss: 1129.9917 - acc: 0.3128 - val_loss: 1114.8202 - val_acc: 0.3256\n",
      "Epoch 13/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1129.0357 - acc: 0.3127 - val_loss: 1117.3628 - val_acc: 0.3260\n",
      "Epoch 14/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1128.1491 - acc: 0.3124 - val_loss: 1112.1401 - val_acc: 0.3063\n",
      "Epoch 15/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1127.4108 - acc: 0.3121 - val_loss: 1111.4834 - val_acc: 0.3227\n",
      "Epoch 16/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1126.7713 - acc: 0.3117 - val_loss: 1118.8047 - val_acc: 0.2931\n",
      "Epoch 17/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1126.2061 - acc: 0.3119 - val_loss: 1111.6418 - val_acc: 0.2989\n",
      "Epoch 18/200\n",
      "508330/508330 [==============================] - 21s 41us/step - loss: 1125.6900 - acc: 0.3117 - val_loss: 1109.6988 - val_acc: 0.3157\n",
      "Epoch 19/200\n",
      "508330/508330 [==============================] - 21s 41us/step - loss: 1125.2560 - acc: 0.3114 - val_loss: 1108.8525 - val_acc: 0.3126\n",
      "Epoch 20/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1124.8578 - acc: 0.3113 - val_loss: 1108.8700 - val_acc: 0.3176\n",
      "Epoch 21/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1124.4017 - acc: 0.3108 - val_loss: 1115.2419 - val_acc: 0.2886\n",
      "Epoch 22/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1124.0835 - acc: 0.3110 - val_loss: 1108.5818 - val_acc: 0.3198\n",
      "Epoch 23/200\n",
      "508330/508330 [==============================] - 20s 40us/step - loss: 1123.7057 - acc: 0.3106 - val_loss: 1110.2147 - val_acc: 0.3255\n",
      "Epoch 24/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1123.3852 - acc: 0.3108 - val_loss: 1108.5599 - val_acc: 0.3039\n",
      "Epoch 25/200\n",
      "508330/508330 [==============================] - 21s 41us/step - loss: 1123.0019 - acc: 0.3105 - val_loss: 1107.7471 - val_acc: 0.3094\n",
      "Epoch 26/200\n",
      "508330/508330 [==============================] - 21s 40us/step - loss: 1122.7386 - acc: 0.3107 - val_loss: 1112.3817 - val_acc: 0.2923\n",
      "Epoch 27/200\n",
      "508330/508330 [==============================] - 21s 41us/step - loss: 1122.3129 - acc: 0.3089 - val_loss: 1107.4937 - val_acc: 0.3011\n",
      "Epoch 28/200\n",
      "508330/508330 [==============================] - 21s 41us/step - loss: 1122.0088 - acc: 0.3070 - val_loss: 1107.8596 - val_acc: 0.3226\n",
      "Epoch 29/200\n",
      "508330/508330 [==============================] - 20s 39us/step - loss: 1121.6650 - acc: 0.3061 - val_loss: 1107.8157 - val_acc: 0.3213\n",
      "Epoch 30/200\n",
      "508330/508330 [==============================] - 19s 38us/step - loss: 1121.3918 - acc: 0.3064 - val_loss: 1106.9973 - val_acc: 0.2970\n",
      "Epoch 31/200\n",
      "508330/508330 [==============================] - 19s 38us/step - loss: 1121.0899 - acc: 0.3063 - val_loss: 1107.2246 - val_acc: 0.3217\n",
      "Epoch 32/200\n",
      "508330/508330 [==============================] - 19s 38us/step - loss: 1120.8474 - acc: 0.3064 - val_loss: 1106.0888 - val_acc: 0.3105\n",
      "Epoch 33/200\n",
      "508330/508330 [==============================] - 19s 38us/step - loss: 1120.6086 - acc: 0.3063 - val_loss: 1106.7408 - val_acc: 0.3206\n",
      "Epoch 34/200\n",
      "508330/508330 [==============================] - 20s 39us/step - loss: 1120.3521 - acc: 0.3063 - val_loss: 1106.1838 - val_acc: 0.3015\n",
      "Epoch 35/200\n",
      "508330/508330 [==============================] - 20s 38us/step - loss: 1120.0963 - acc: 0.3060 - val_loss: 1105.6036 - val_acc: 0.3086\n",
      "Epoch 36/200\n",
      "507264/508330 [============================>.] - ETA: 0s - loss: 1119.9137 - acc: 0.3062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-1ee49c9eff7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                          validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                                          verbose=0)\n\u001b[0m\u001b[1;32m    219\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 200\n",
    "history = model.fit(X_train, y_train,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         verbose=1,\n",
    "                         validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LZtd6JnBmHfP",
    "outputId": "0e578dd4-dfb8-4413-8e41-e321aa1edc11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1116.6540 - acc: 0.3065\n",
      "Epoch 2/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1116.4964 - acc: 0.3069\n",
      "Epoch 3/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1116.3985 - acc: 0.3067\n",
      "Epoch 4/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1116.3235 - acc: 0.3068\n",
      "Epoch 5/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1116.2138 - acc: 0.3068\n",
      "Epoch 6/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1116.1073 - acc: 0.3067\n",
      "Epoch 7/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1116.0358 - acc: 0.3069\n",
      "Epoch 8/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.9587 - acc: 0.3069\n",
      "Epoch 9/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1115.8705 - acc: 0.3068\n",
      "Epoch 10/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.7595 - acc: 0.3066\n",
      "Epoch 11/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.7206 - acc: 0.3063\n",
      "Epoch 12/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1115.6303 - acc: 0.3065\n",
      "Epoch 13/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.5628 - acc: 0.3063\n",
      "Epoch 14/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.4735 - acc: 0.3064\n",
      "Epoch 15/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.4053 - acc: 0.3065\n",
      "Epoch 16/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.3419 - acc: 0.3065\n",
      "Epoch 17/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.2377 - acc: 0.3064\n",
      "Epoch 18/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.1638 - acc: 0.3065\n",
      "Epoch 19/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1115.1304 - acc: 0.3064\n",
      "Epoch 20/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1115.0461 - acc: 0.3061\n",
      "Epoch 21/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1114.9732 - acc: 0.3060\n",
      "Epoch 22/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.9056 - acc: 0.3060\n",
      "Epoch 23/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.8221 - acc: 0.3060\n",
      "Epoch 24/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.7798 - acc: 0.3057\n",
      "Epoch 25/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.6983 - acc: 0.3032\n",
      "Epoch 26/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.6319 - acc: 0.3016\n",
      "Epoch 27/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.5808 - acc: 0.3013\n",
      "Epoch 28/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.5012 - acc: 0.3009\n",
      "Epoch 29/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.4470 - acc: 0.3007\n",
      "Epoch 30/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.3918 - acc: 0.3007\n",
      "Epoch 31/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1114.3299 - acc: 0.3013\n",
      "Epoch 32/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1114.2879 - acc: 0.3012\n",
      "Epoch 33/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1114.1938 - acc: 0.3011\n",
      "Epoch 34/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1114.1507 - acc: 0.3010\n",
      "Epoch 35/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1114.0822 - acc: 0.3011\n",
      "Epoch 36/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1114.0261 - acc: 0.3010\n",
      "Epoch 37/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.9718 - acc: 0.3011\n",
      "Epoch 38/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.9041 - acc: 0.3011\n",
      "Epoch 39/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.8591 - acc: 0.3011\n",
      "Epoch 40/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.8160 - acc: 0.3010\n",
      "Epoch 41/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.7211 - acc: 0.3012\n",
      "Epoch 42/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.6795 - acc: 0.3010\n",
      "Epoch 43/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.6204 - acc: 0.3013\n",
      "Epoch 44/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.5664 - acc: 0.3013\n",
      "Epoch 45/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.5386 - acc: 0.3009\n",
      "Epoch 46/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.4646 - acc: 0.3013\n",
      "Epoch 47/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.4026 - acc: 0.3012\n",
      "Epoch 48/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.3616 - acc: 0.3014\n",
      "Epoch 49/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.3079 - acc: 0.3014\n",
      "Epoch 50/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.2574 - acc: 0.3014\n",
      "Epoch 51/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.1907 - acc: 0.3010\n",
      "Epoch 52/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.1418 - acc: 0.3013\n",
      "Epoch 53/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1113.0869 - acc: 0.3014\n",
      "Epoch 54/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1113.0502 - acc: 0.3011\n",
      "Epoch 55/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.9917 - acc: 0.3014\n",
      "Epoch 56/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.9369 - acc: 0.3011\n",
      "Epoch 57/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.8781 - acc: 0.3009\n",
      "Epoch 58/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.8268 - acc: 0.3008\n",
      "Epoch 59/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.7744 - acc: 0.3008\n",
      "Epoch 60/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.7398 - acc: 0.3007\n",
      "Epoch 61/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.6939 - acc: 0.3010\n",
      "Epoch 62/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.6355 - acc: 0.3013\n",
      "Epoch 63/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.5767 - acc: 0.3010\n",
      "Epoch 64/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.5436 - acc: 0.3010\n",
      "Epoch 65/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.4772 - acc: 0.3010\n",
      "Epoch 66/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.4541 - acc: 0.2993\n",
      "Epoch 67/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.3818 - acc: 0.2882\n",
      "Epoch 68/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.3306 - acc: 0.2862\n",
      "Epoch 69/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.2732 - acc: 0.2859\n",
      "Epoch 70/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.2526 - acc: 0.2859\n",
      "Epoch 71/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.2027 - acc: 0.2861\n",
      "Epoch 72/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.1637 - acc: 0.2857\n",
      "Epoch 73/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1112.1276 - acc: 0.2858\n",
      "Epoch 74/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.0630 - acc: 0.2859\n",
      "Epoch 75/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1112.0304 - acc: 0.2861\n",
      "Epoch 76/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1111.9851 - acc: 0.2858\n",
      "Epoch 77/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.9643 - acc: 0.2859\n",
      "Epoch 78/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.9099 - acc: 0.2859\n",
      "Epoch 79/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.8722 - acc: 0.2861\n",
      "Epoch 80/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.8452 - acc: 0.2863\n",
      "Epoch 81/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.8120 - acc: 0.2862\n",
      "Epoch 82/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1111.7587 - acc: 0.2864\n",
      "Epoch 83/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.7225 - acc: 0.2863\n",
      "Epoch 84/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1111.6784 - acc: 0.2863\n",
      "Epoch 85/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.6407 - acc: 0.2868\n",
      "Epoch 86/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.6080 - acc: 0.2863\n",
      "Epoch 87/200\n",
      "635413/635413 [==============================] - 12s 18us/step - loss: 1111.5559 - acc: 0.2865\n",
      "Epoch 88/200\n",
      "635413/635413 [==============================] - 12s 19us/step - loss: 1111.5328 - acc: 0.2866\n",
      "Epoch 89/200\n",
      " 48384/635413 [=>............................] - ETA: 11s - loss: 1053.0414 - acc: 0.2837"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-88cec7226b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                          )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 200\n",
    "history = model.fit(X_values, Y,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         verbose=1,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Plxe4fknhSao"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(Data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sa13JHe3kszD",
    "outputId": "e35755f8-d662-4a2b-e8b6-643eafc2508e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106.6741699329218"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "QvEjTt3qkxz-",
    "outputId": "41479b63-278e-4d4f-f3a2-1c5aad2cc183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00],\n",
       "       [4.6323601e+01],\n",
       "       [0.0000000e+00],\n",
       "       [2.9771949e+04],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [2.0430628e+01],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [7.3187904e+01],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [2.3232985e+02],\n",
       "       [0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[200:220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "5Tc2eLgrk1Gi",
    "outputId": "aef7c7b2-e393-41e3-fa02-0325dee62deb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   21,    10,   161, 10393,     0,     0,     0,     2,     9,\n",
       "           0,     0,     2,     0,     5,     0,     4,     0,    17,\n",
       "         348,     0])"
      ]
     },
     "execution_count": 94,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[200:220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ymHlKFPPk5L5"
   },
   "outputs": [],
   "source": [
    "word_vector = D_train.iloc[:,14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dGoJIYfMs7lc",
    "outputId": "4fba748f-20f4-4914-9227-f065ccf8ea05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536, 1024)"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,n = word_vector.shape\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "Gu2sy0o3r0po",
    "outputId": "be38284d-af9c-4fdf-c11f-294e6e1b441a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>V40</th>\n",
       "      <th>...</th>\n",
       "      <th>V985</th>\n",
       "      <th>V986</th>\n",
       "      <th>V987</th>\n",
       "      <th>V988</th>\n",
       "      <th>V989</th>\n",
       "      <th>V990</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "      <th>V1001</th>\n",
       "      <th>V1002</th>\n",
       "      <th>V1003</th>\n",
       "      <th>V1004</th>\n",
       "      <th>V1005</th>\n",
       "      <th>V1006</th>\n",
       "      <th>V1007</th>\n",
       "      <th>V1008</th>\n",
       "      <th>V1009</th>\n",
       "      <th>V1010</th>\n",
       "      <th>V1011</th>\n",
       "      <th>V1012</th>\n",
       "      <th>V1013</th>\n",
       "      <th>V1014</th>\n",
       "      <th>V1015</th>\n",
       "      <th>V1016</th>\n",
       "      <th>V1017</th>\n",
       "      <th>V1018</th>\n",
       "      <th>V1019</th>\n",
       "      <th>V1020</th>\n",
       "      <th>V1021</th>\n",
       "      <th>V1022</th>\n",
       "      <th>V1023</th>\n",
       "      <th>V1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "      <td>5536.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>0.011245</td>\n",
       "      <td>0.059855</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.044270</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.027567</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.024895</td>\n",
       "      <td>0.010980</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.042604</td>\n",
       "      <td>0.038158</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>0.017065</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>0.013880</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.024753</td>\n",
       "      <td>0.010955</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.014206</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>0.023864</td>\n",
       "      <td>0.018314</td>\n",
       "      <td>0.023150</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.016516</td>\n",
       "      <td>0.041989</td>\n",
       "      <td>0.032254</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.021443</td>\n",
       "      <td>0.018738</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.025442</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.015429</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.031793</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.025639</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>0.030124</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.036011</td>\n",
       "      <td>0.029263</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>0.022588</td>\n",
       "      <td>0.028954</td>\n",
       "      <td>0.016549</td>\n",
       "      <td>0.027529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.010882</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>0.010438</td>\n",
       "      <td>0.014416</td>\n",
       "      <td>0.014008</td>\n",
       "      <td>0.032841</td>\n",
       "      <td>0.014975</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.011826</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.020108</td>\n",
       "      <td>0.014428</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>0.011273</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.007844</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.018954</td>\n",
       "      <td>0.031928</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.014884</td>\n",
       "      <td>0.013681</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.013573</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>0.007003</td>\n",
       "      <td>0.012823</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.019363</td>\n",
       "      <td>0.033695</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.013380</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>0.007582</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>0.015730</td>\n",
       "      <td>0.012709</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>0.012511</td>\n",
       "      <td>0.011833</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.012016</td>\n",
       "      <td>0.028394</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>0.007509</td>\n",
       "      <td>0.006288</td>\n",
       "      <td>0.022656</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>0.013307</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.014678</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.012155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.008949</td>\n",
       "      <td>-0.002177</td>\n",
       "      <td>-0.014520</td>\n",
       "      <td>-0.004323</td>\n",
       "      <td>-0.028288</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.006689</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>-0.008393</td>\n",
       "      <td>-0.005328</td>\n",
       "      <td>-0.007194</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>-0.013965</td>\n",
       "      <td>-0.003646</td>\n",
       "      <td>-0.006471</td>\n",
       "      <td>-0.008644</td>\n",
       "      <td>-0.002362</td>\n",
       "      <td>-0.002764</td>\n",
       "      <td>-0.005229</td>\n",
       "      <td>-0.006172</td>\n",
       "      <td>-0.011256</td>\n",
       "      <td>-0.001554</td>\n",
       "      <td>-0.011374</td>\n",
       "      <td>-0.007767</td>\n",
       "      <td>-0.003202</td>\n",
       "      <td>-0.004765</td>\n",
       "      <td>-0.003972</td>\n",
       "      <td>-0.010695</td>\n",
       "      <td>-0.019191</td>\n",
       "      <td>-0.004275</td>\n",
       "      <td>-0.001886</td>\n",
       "      <td>-0.006177</td>\n",
       "      <td>-0.020467</td>\n",
       "      <td>-0.004988</td>\n",
       "      <td>-0.006561</td>\n",
       "      <td>-0.006959</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>-0.005024</td>\n",
       "      <td>-0.017808</td>\n",
       "      <td>-0.009987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002557</td>\n",
       "      <td>-0.006237</td>\n",
       "      <td>-0.003852</td>\n",
       "      <td>-0.012573</td>\n",
       "      <td>-0.007819</td>\n",
       "      <td>-0.002968</td>\n",
       "      <td>-0.013919</td>\n",
       "      <td>-0.008601</td>\n",
       "      <td>-0.016584</td>\n",
       "      <td>-0.008514</td>\n",
       "      <td>-0.003818</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>-0.010675</td>\n",
       "      <td>-0.005668</td>\n",
       "      <td>-0.011003</td>\n",
       "      <td>-0.016346</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.005261</td>\n",
       "      <td>-0.002250</td>\n",
       "      <td>-0.006256</td>\n",
       "      <td>-0.005940</td>\n",
       "      <td>-0.007426</td>\n",
       "      <td>-0.005071</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.003818</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>-0.003843</td>\n",
       "      <td>-0.008632</td>\n",
       "      <td>-0.004487</td>\n",
       "      <td>-0.011011</td>\n",
       "      <td>-0.006560</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.006889</td>\n",
       "      <td>-0.007179</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>-0.010453</td>\n",
       "      <td>-0.003065</td>\n",
       "      <td>-0.006794</td>\n",
       "      <td>-0.017138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.003204</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.025259</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.011677</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.016912</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.030679</td>\n",
       "      <td>0.015844</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.015677</td>\n",
       "      <td>-0.002560</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.023056</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>0.009626</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000672</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.017972</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.020454</td>\n",
       "      <td>0.012408</td>\n",
       "      <td>0.017466</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>-0.001464</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.018835</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.018917</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.019774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.009172</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>0.010067</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.071056</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>0.046902</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.022077</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.024110</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.045401</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.011697</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.024421</td>\n",
       "      <td>0.014618</td>\n",
       "      <td>0.022568</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017491</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>0.034882</td>\n",
       "      <td>0.033325</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.026675</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>0.008786</td>\n",
       "      <td>0.033525</td>\n",
       "      <td>0.020560</td>\n",
       "      <td>0.025292</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.036651</td>\n",
       "      <td>0.018334</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>0.015787</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.016819</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.015777</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.016686</td>\n",
       "      <td>0.009895</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>0.087391</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.057576</td>\n",
       "      <td>0.011867</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.041625</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.032299</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>0.055859</td>\n",
       "      <td>0.050671</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.012257</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.037413</td>\n",
       "      <td>0.010332</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>0.030697</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>0.029909</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026902</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.022487</td>\n",
       "      <td>0.067297</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.027173</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.032993</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.023021</td>\n",
       "      <td>0.013291</td>\n",
       "      <td>0.043758</td>\n",
       "      <td>0.029431</td>\n",
       "      <td>0.033550</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.008424</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.050224</td>\n",
       "      <td>0.051449</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.009695</td>\n",
       "      <td>0.030448</td>\n",
       "      <td>0.039091</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.035927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.062851</td>\n",
       "      <td>0.110767</td>\n",
       "      <td>0.239115</td>\n",
       "      <td>0.092428</td>\n",
       "      <td>0.097574</td>\n",
       "      <td>0.109939</td>\n",
       "      <td>0.098943</td>\n",
       "      <td>0.143397</td>\n",
       "      <td>0.093653</td>\n",
       "      <td>0.107693</td>\n",
       "      <td>0.088904</td>\n",
       "      <td>0.084364</td>\n",
       "      <td>0.120729</td>\n",
       "      <td>0.109899</td>\n",
       "      <td>0.131609</td>\n",
       "      <td>0.076376</td>\n",
       "      <td>0.077455</td>\n",
       "      <td>0.089508</td>\n",
       "      <td>0.097789</td>\n",
       "      <td>0.075228</td>\n",
       "      <td>0.108855</td>\n",
       "      <td>0.170772</td>\n",
       "      <td>0.237792</td>\n",
       "      <td>0.073645</td>\n",
       "      <td>0.101224</td>\n",
       "      <td>0.101332</td>\n",
       "      <td>0.083317</td>\n",
       "      <td>0.090915</td>\n",
       "      <td>0.140295</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>0.077929</td>\n",
       "      <td>0.091753</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>0.134971</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>0.053714</td>\n",
       "      <td>0.087316</td>\n",
       "      <td>0.068592</td>\n",
       "      <td>0.088672</td>\n",
       "      <td>0.116694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082742</td>\n",
       "      <td>0.105215</td>\n",
       "      <td>0.097987</td>\n",
       "      <td>0.161603</td>\n",
       "      <td>0.113614</td>\n",
       "      <td>0.168006</td>\n",
       "      <td>0.080774</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.080815</td>\n",
       "      <td>0.119513</td>\n",
       "      <td>0.142894</td>\n",
       "      <td>0.095550</td>\n",
       "      <td>0.098191</td>\n",
       "      <td>0.098920</td>\n",
       "      <td>0.088579</td>\n",
       "      <td>0.089750</td>\n",
       "      <td>0.132917</td>\n",
       "      <td>0.093646</td>\n",
       "      <td>0.070948</td>\n",
       "      <td>0.074711</td>\n",
       "      <td>0.081236</td>\n",
       "      <td>0.074234</td>\n",
       "      <td>0.121505</td>\n",
       "      <td>0.096526</td>\n",
       "      <td>0.067057</td>\n",
       "      <td>0.106231</td>\n",
       "      <td>0.159689</td>\n",
       "      <td>0.067478</td>\n",
       "      <td>0.067304</td>\n",
       "      <td>0.128008</td>\n",
       "      <td>0.160952</td>\n",
       "      <td>0.110284</td>\n",
       "      <td>0.105644</td>\n",
       "      <td>0.108339</td>\n",
       "      <td>0.087524</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.093731</td>\n",
       "      <td>0.081818</td>\n",
       "      <td>0.089095</td>\n",
       "      <td>0.071600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                V1           V2  ...        V1023        V1024\n",
       "count  5536.000000  5536.000000  ...  5536.000000  5536.000000\n",
       "mean      0.011405     0.001566  ...     0.016549     0.027529\n",
       "std       0.010882     0.005966  ...     0.009375     0.012155\n",
       "min      -0.008949    -0.002177  ...    -0.006794    -0.017138\n",
       "25%       0.003204    -0.000026  ...     0.010491     0.019774\n",
       "50%       0.009172    -0.000007  ...     0.015787     0.028600\n",
       "75%       0.016819     0.000262  ...     0.021814     0.035927\n",
       "max       0.062851     0.110767  ...     0.089095     0.071600\n",
       "\n",
       "[8 rows x 1024 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "APJsrEFJr1tf"
   },
   "outputs": [],
   "source": [
    "train_pred = model.predict(X_values[-m:])\n",
    "residuals = Y[-m:] - np.squeeze(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WxnE2byOtKMW"
   },
   "outputs": [],
   "source": [
    "residual_X = word_vector.values\n",
    "residual_Y = residuals\n",
    "resX_train, resX_test, resy_train, resy_test = train_test_split(residual_X,residual_Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8xEuJ03Jv8XE",
    "outputId": "5805cf9a-2baa-4a92-af09-f9c826a088d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536,)"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vCKBrlvBwKFT",
    "outputId": "6257894a-ccee-4777-d629-efee0731183b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536,)"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[-m:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "7JHMHqhztLiv",
    "outputId": "cd2f4f31-90a0-48e2-b886-4a773f0dd5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 304,609\n",
      "Trainable params: 304,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = n\n",
    "residual_model = Sequential()\n",
    "residual_model.add(Dense(256, input_dim = input_dim,activation='relu'))\n",
    "residual_model.add(Dense(128, activation='relu'))\n",
    "residual_model.add(Dense(64, activation='relu'))\n",
    "residual_model.add(Dense(16, activation='relu'))\n",
    "residual_model.add(Dense(1))\n",
    "residual_model.compile(\n",
    "    loss=keras.losses.mean_absolute_error,\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "residual_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0mTelpElt_-b",
    "outputId": "21d7317d-ef53-46d2-a020-ad5b05aaec3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4428 samples, validate on 1108 samples\n",
      "Epoch 1/100\n",
      "4428/4428 [==============================] - 1s 124us/step - loss: 13139.4847 - acc: 0.0400 - val_loss: 10314.1405 - val_acc: 0.0659\n",
      "Epoch 2/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 13118.1722 - acc: 0.1527 - val_loss: 10289.8195 - val_acc: 0.1949\n",
      "Epoch 3/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 13096.9084 - acc: 0.1893 - val_loss: 10278.3699 - val_acc: 0.2148\n",
      "Epoch 4/100\n",
      "4428/4428 [==============================] - 0s 53us/step - loss: 13085.9447 - acc: 0.2037 - val_loss: 10273.7762 - val_acc: 0.2175\n",
      "Epoch 5/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 13077.4115 - acc: 0.2096 - val_loss: 10272.7077 - val_acc: 0.2085\n",
      "Epoch 6/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 13070.0804 - acc: 0.2089 - val_loss: 10267.8921 - val_acc: 0.2220\n",
      "Epoch 7/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 13062.7647 - acc: 0.2130 - val_loss: 10265.0754 - val_acc: 0.2229\n",
      "Epoch 8/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 13055.4411 - acc: 0.2134 - val_loss: 10264.5538 - val_acc: 0.2166\n",
      "Epoch 9/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 13049.5388 - acc: 0.2159 - val_loss: 10263.3220 - val_acc: 0.2229\n",
      "Epoch 10/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 13044.4942 - acc: 0.2150 - val_loss: 10264.2051 - val_acc: 0.2256\n",
      "Epoch 11/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 13036.1154 - acc: 0.2154 - val_loss: 10268.1104 - val_acc: 0.2148\n",
      "Epoch 12/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 13029.8394 - acc: 0.2186 - val_loss: 10271.4345 - val_acc: 0.2166\n",
      "Epoch 13/100\n",
      "4428/4428 [==============================] - 0s 52us/step - loss: 13023.4182 - acc: 0.2184 - val_loss: 10265.1219 - val_acc: 0.2220\n",
      "Epoch 14/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 13013.9774 - acc: 0.2168 - val_loss: 10263.8370 - val_acc: 0.2329\n",
      "Epoch 15/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 13008.9411 - acc: 0.2173 - val_loss: 10263.8891 - val_acc: 0.2301\n",
      "Epoch 16/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12997.0472 - acc: 0.2179 - val_loss: 10268.6273 - val_acc: 0.2211\n",
      "Epoch 17/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12990.9903 - acc: 0.2179 - val_loss: 10266.5133 - val_acc: 0.2211\n",
      "Epoch 18/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12981.8519 - acc: 0.2179 - val_loss: 10270.7012 - val_acc: 0.2211\n",
      "Epoch 19/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12975.4185 - acc: 0.2184 - val_loss: 10265.8915 - val_acc: 0.2229\n",
      "Epoch 20/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12971.3764 - acc: 0.2173 - val_loss: 10268.9778 - val_acc: 0.2220\n",
      "Epoch 21/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12960.0575 - acc: 0.2179 - val_loss: 10269.4350 - val_acc: 0.2220\n",
      "Epoch 22/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12949.1715 - acc: 0.2177 - val_loss: 10277.4163 - val_acc: 0.2211\n",
      "Epoch 23/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12941.9995 - acc: 0.2170 - val_loss: 10283.3108 - val_acc: 0.2193\n",
      "Epoch 24/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12933.6930 - acc: 0.2166 - val_loss: 10274.4285 - val_acc: 0.2229\n",
      "Epoch 25/100\n",
      "4428/4428 [==============================] - 0s 45us/step - loss: 12921.1324 - acc: 0.2173 - val_loss: 10285.6425 - val_acc: 0.2211\n",
      "Epoch 26/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12917.7357 - acc: 0.2175 - val_loss: 10309.7265 - val_acc: 0.2184\n",
      "Epoch 27/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12900.9462 - acc: 0.2150 - val_loss: 10281.6991 - val_acc: 0.2238\n",
      "Epoch 28/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12896.3750 - acc: 0.2177 - val_loss: 10283.6579 - val_acc: 0.2247\n",
      "Epoch 29/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12881.7682 - acc: 0.2168 - val_loss: 10279.9616 - val_acc: 0.2283\n",
      "Epoch 30/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12876.0154 - acc: 0.2168 - val_loss: 10339.3054 - val_acc: 0.2175\n",
      "Epoch 31/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12872.7231 - acc: 0.2184 - val_loss: 10316.5170 - val_acc: 0.2211\n",
      "Epoch 32/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12852.6102 - acc: 0.2188 - val_loss: 10296.3119 - val_acc: 0.2247\n",
      "Epoch 33/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12842.6822 - acc: 0.2188 - val_loss: 10298.9318 - val_acc: 0.2238\n",
      "Epoch 34/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12834.4699 - acc: 0.2164 - val_loss: 10369.3423 - val_acc: 0.2166\n",
      "Epoch 35/100\n",
      "4428/4428 [==============================] - 0s 45us/step - loss: 12823.3138 - acc: 0.2168 - val_loss: 10345.3961 - val_acc: 0.2184\n",
      "Epoch 36/100\n",
      "4428/4428 [==============================] - 0s 45us/step - loss: 12814.9957 - acc: 0.2173 - val_loss: 10309.2631 - val_acc: 0.2247\n",
      "Epoch 37/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12798.6218 - acc: 0.2170 - val_loss: 10314.9759 - val_acc: 0.2229\n",
      "Epoch 38/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12787.4626 - acc: 0.2177 - val_loss: 10374.1976 - val_acc: 0.2166\n",
      "Epoch 39/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12781.2597 - acc: 0.2173 - val_loss: 10326.8688 - val_acc: 0.2211\n",
      "Epoch 40/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12771.3516 - acc: 0.2177 - val_loss: 10370.1896 - val_acc: 0.2184\n",
      "Epoch 41/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12768.7137 - acc: 0.2164 - val_loss: 10357.7507 - val_acc: 0.2202\n",
      "Epoch 42/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12743.8633 - acc: 0.2186 - val_loss: 10397.7012 - val_acc: 0.2157\n",
      "Epoch 43/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12735.7704 - acc: 0.2166 - val_loss: 10380.8036 - val_acc: 0.2193\n",
      "Epoch 44/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12718.2310 - acc: 0.2168 - val_loss: 10423.8295 - val_acc: 0.2157\n",
      "Epoch 45/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12701.2874 - acc: 0.2161 - val_loss: 10355.9213 - val_acc: 0.2220\n",
      "Epoch 46/100\n",
      "4428/4428 [==============================] - 0s 52us/step - loss: 12697.7137 - acc: 0.2152 - val_loss: 10396.7025 - val_acc: 0.2175\n",
      "Epoch 47/100\n",
      "4428/4428 [==============================] - 0s 52us/step - loss: 12682.9517 - acc: 0.2177 - val_loss: 10446.6297 - val_acc: 0.2148\n",
      "Epoch 48/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12674.4443 - acc: 0.2170 - val_loss: 10463.0215 - val_acc: 0.2148\n",
      "Epoch 49/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12663.3309 - acc: 0.2177 - val_loss: 10374.5326 - val_acc: 0.2238\n",
      "Epoch 50/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12652.8594 - acc: 0.2157 - val_loss: 10399.2798 - val_acc: 0.2202\n",
      "Epoch 51/100\n",
      "4428/4428 [==============================] - 0s 45us/step - loss: 12630.7224 - acc: 0.2170 - val_loss: 10398.7442 - val_acc: 0.2229\n",
      "Epoch 52/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12617.3274 - acc: 0.2164 - val_loss: 10342.7772 - val_acc: 0.2283\n",
      "Epoch 53/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12624.5391 - acc: 0.2132 - val_loss: 10379.2322 - val_acc: 0.2256\n",
      "Epoch 54/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12591.9681 - acc: 0.2170 - val_loss: 10421.0520 - val_acc: 0.2220\n",
      "Epoch 55/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12594.1507 - acc: 0.2175 - val_loss: 10468.0381 - val_acc: 0.2202\n",
      "Epoch 56/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12571.3600 - acc: 0.2148 - val_loss: 10470.6748 - val_acc: 0.2049\n",
      "Epoch 57/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12557.6057 - acc: 0.2148 - val_loss: 10480.6556 - val_acc: 0.2184\n",
      "Epoch 58/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12564.0796 - acc: 0.2132 - val_loss: 10498.3705 - val_acc: 0.1986\n",
      "Epoch 59/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12542.4728 - acc: 0.2166 - val_loss: 10462.2717 - val_acc: 0.2211\n",
      "Epoch 60/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12530.9881 - acc: 0.2139 - val_loss: 10434.8883 - val_acc: 0.2238\n",
      "Epoch 61/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12510.7720 - acc: 0.2139 - val_loss: 10410.6362 - val_acc: 0.2256\n",
      "Epoch 62/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12503.4856 - acc: 0.2143 - val_loss: 10521.2655 - val_acc: 0.2211\n",
      "Epoch 63/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12487.8098 - acc: 0.2166 - val_loss: 10602.1194 - val_acc: 0.2184\n",
      "Epoch 64/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12484.6178 - acc: 0.2145 - val_loss: 10421.7997 - val_acc: 0.2247\n",
      "Epoch 65/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12469.7439 - acc: 0.2098 - val_loss: 10472.4820 - val_acc: 0.2238\n",
      "Epoch 66/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12450.9065 - acc: 0.2141 - val_loss: 10426.4466 - val_acc: 0.2265\n",
      "Epoch 67/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12443.7054 - acc: 0.2139 - val_loss: 10483.9642 - val_acc: 0.2247\n",
      "Epoch 68/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12436.3437 - acc: 0.2159 - val_loss: 10531.3649 - val_acc: 0.2247\n",
      "Epoch 69/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12414.9789 - acc: 0.2112 - val_loss: 10541.5289 - val_acc: 0.2265\n",
      "Epoch 70/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12412.7131 - acc: 0.2157 - val_loss: 10481.1158 - val_acc: 0.2247\n",
      "Epoch 71/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12386.5678 - acc: 0.2164 - val_loss: 10488.7840 - val_acc: 0.2238\n",
      "Epoch 72/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12388.2745 - acc: 0.2166 - val_loss: 10460.2575 - val_acc: 0.2193\n",
      "Epoch 73/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12367.9866 - acc: 0.2177 - val_loss: 10487.2917 - val_acc: 0.2247\n",
      "Epoch 74/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12353.0776 - acc: 0.2145 - val_loss: 10620.1820 - val_acc: 0.2247\n",
      "Epoch 75/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12352.8525 - acc: 0.2179 - val_loss: 10475.8393 - val_acc: 0.2274\n",
      "Epoch 76/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12328.6584 - acc: 0.2114 - val_loss: 10523.5516 - val_acc: 0.2238\n",
      "Epoch 77/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12324.5857 - acc: 0.2062 - val_loss: 10443.1263 - val_acc: 0.1146\n",
      "Epoch 78/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12307.6136 - acc: 0.2143 - val_loss: 10522.3734 - val_acc: 0.2166\n",
      "Epoch 79/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12284.3883 - acc: 0.2161 - val_loss: 10647.0284 - val_acc: 0.2247\n",
      "Epoch 80/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12277.8965 - acc: 0.2154 - val_loss: 10547.8379 - val_acc: 0.2256\n",
      "Epoch 81/100\n",
      "4428/4428 [==============================] - 0s 45us/step - loss: 12256.5063 - acc: 0.2143 - val_loss: 10460.3232 - val_acc: 0.2274\n",
      "Epoch 82/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12242.7225 - acc: 0.2175 - val_loss: 10685.5178 - val_acc: 0.2247\n",
      "Epoch 83/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12245.5150 - acc: 0.2112 - val_loss: 10515.6779 - val_acc: 0.2265\n",
      "Epoch 84/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12240.2765 - acc: 0.2109 - val_loss: 10559.1160 - val_acc: 0.2247\n",
      "Epoch 85/100\n",
      "4428/4428 [==============================] - 0s 51us/step - loss: 12208.0185 - acc: 0.2109 - val_loss: 10530.4604 - val_acc: 0.2274\n",
      "Epoch 86/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12204.3201 - acc: 0.2139 - val_loss: 10626.6627 - val_acc: 0.1399\n",
      "Epoch 87/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12190.6327 - acc: 0.2107 - val_loss: 10488.8471 - val_acc: 0.2283\n",
      "Epoch 88/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12175.0823 - acc: 0.2091 - val_loss: 10626.0138 - val_acc: 0.2247\n",
      "Epoch 89/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12163.9067 - acc: 0.2132 - val_loss: 10705.9837 - val_acc: 0.2247\n",
      "Epoch 90/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12120.1097 - acc: 0.2096 - val_loss: 10676.9798 - val_acc: 0.2256\n",
      "Epoch 91/100\n",
      "4428/4428 [==============================] - 0s 49us/step - loss: 12116.9134 - acc: 0.2152 - val_loss: 10751.2730 - val_acc: 0.2247\n",
      "Epoch 92/100\n",
      "4428/4428 [==============================] - 0s 46us/step - loss: 12116.6687 - acc: 0.2148 - val_loss: 10607.3361 - val_acc: 0.0560\n",
      "Epoch 93/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12085.3322 - acc: 0.2114 - val_loss: 10602.9222 - val_acc: 0.2256\n",
      "Epoch 94/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12066.8618 - acc: 0.2148 - val_loss: 10719.3465 - val_acc: 0.2247\n",
      "Epoch 95/100\n",
      "4428/4428 [==============================] - 0s 50us/step - loss: 12051.4566 - acc: 0.2132 - val_loss: 10654.8169 - val_acc: 0.2274\n",
      "Epoch 96/100\n",
      "4428/4428 [==============================] - 0s 52us/step - loss: 12054.4508 - acc: 0.2175 - val_loss: 10709.8985 - val_acc: 0.2256\n",
      "Epoch 97/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12039.2336 - acc: 0.2152 - val_loss: 10598.9528 - val_acc: 0.2265\n",
      "Epoch 98/100\n",
      "4428/4428 [==============================] - 0s 48us/step - loss: 12028.8242 - acc: 0.2130 - val_loss: 10714.8524 - val_acc: 0.2256\n",
      "Epoch 99/100\n",
      "4428/4428 [==============================] - 0s 52us/step - loss: 11996.8097 - acc: 0.2087 - val_loss: 10928.9253 - val_acc: 0.1191\n",
      "Epoch 100/100\n",
      "4428/4428 [==============================] - 0s 47us/step - loss: 12002.5921 - acc: 0.2139 - val_loss: 10846.6358 - val_acc: 0.2238\n",
      "Test loss: 10846.635726322766\n",
      "Test accuracy: 0.22382671490903366\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "history_res = residual_model.fit(resX_train, resy_train,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         verbose=1,\n",
    "                         validation_data=(resX_test, resy_test))\n",
    "score_res = residual_model.evaluate(resX_test, resy_test, verbose=0)\n",
    "print('Test loss:', score_res[0])\n",
    "print('Test accuracy:', score_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ENJldN6q1qUx",
    "outputId": "8438cd86-8d8a-420c-dfc3-5fce20d1e5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5536/5536 [==============================] - 1s 112us/step - loss: 12571.9739 - acc: 0.0569\n",
      "Epoch 2/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 12547.1820 - acc: 0.1768\n",
      "Epoch 3/100\n",
      "5536/5536 [==============================] - 0s 56us/step - loss: 12529.4626 - acc: 0.2030\n",
      "Epoch 4/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12521.0830 - acc: 0.2101\n",
      "Epoch 5/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12513.6227 - acc: 0.2142\n",
      "Epoch 6/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12507.0535 - acc: 0.2160\n",
      "Epoch 7/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12501.7205 - acc: 0.2177\n",
      "Epoch 8/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12493.7923 - acc: 0.2197\n",
      "Epoch 9/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 12489.0833 - acc: 0.2220\n",
      "Epoch 10/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12482.3486 - acc: 0.2225\n",
      "Epoch 11/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12476.3819 - acc: 0.2240\n",
      "Epoch 12/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12469.1855 - acc: 0.2231\n",
      "Epoch 13/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12467.1003 - acc: 0.2231\n",
      "Epoch 14/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12458.6505 - acc: 0.2238\n",
      "Epoch 15/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12453.3886 - acc: 0.2225\n",
      "Epoch 16/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 12448.5979 - acc: 0.2233\n",
      "Epoch 17/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12441.7621 - acc: 0.2238\n",
      "Epoch 18/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12436.7090 - acc: 0.2247\n",
      "Epoch 19/100\n",
      "5536/5536 [==============================] - 0s 50us/step - loss: 12430.8857 - acc: 0.2242\n",
      "Epoch 20/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12422.8635 - acc: 0.2240\n",
      "Epoch 21/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12416.8892 - acc: 0.2243\n",
      "Epoch 22/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 12407.9348 - acc: 0.2247\n",
      "Epoch 23/100\n",
      "5536/5536 [==============================] - 0s 46us/step - loss: 12404.8365 - acc: 0.2249\n",
      "Epoch 24/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12396.2293 - acc: 0.2247\n",
      "Epoch 25/100\n",
      "5536/5536 [==============================] - 0s 50us/step - loss: 12388.5599 - acc: 0.2240\n",
      "Epoch 26/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12378.6828 - acc: 0.2240\n",
      "Epoch 27/100\n",
      "5536/5536 [==============================] - 0s 50us/step - loss: 12376.2970 - acc: 0.2242\n",
      "Epoch 28/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 12372.5714 - acc: 0.2224\n",
      "Epoch 29/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 12362.7620 - acc: 0.2220\n",
      "Epoch 30/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12355.0559 - acc: 0.2213\n",
      "Epoch 31/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12346.6418 - acc: 0.2206\n",
      "Epoch 32/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 12342.0412 - acc: 0.2200\n",
      "Epoch 33/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12333.3043 - acc: 0.2204\n",
      "Epoch 34/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12326.4146 - acc: 0.2197\n",
      "Epoch 35/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12317.9892 - acc: 0.2197\n",
      "Epoch 36/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12311.0495 - acc: 0.2188\n",
      "Epoch 37/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12300.6945 - acc: 0.2189\n",
      "Epoch 38/100\n",
      "5536/5536 [==============================] - 0s 50us/step - loss: 12295.4450 - acc: 0.2177\n",
      "Epoch 39/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 12290.4501 - acc: 0.2164\n",
      "Epoch 40/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12277.1089 - acc: 0.2160\n",
      "Epoch 41/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12272.3169 - acc: 0.2164\n",
      "Epoch 42/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12263.5555 - acc: 0.2155\n",
      "Epoch 43/100\n",
      "5536/5536 [==============================] - 0s 50us/step - loss: 12251.4364 - acc: 0.2162\n",
      "Epoch 44/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 12244.7531 - acc: 0.2157\n",
      "Epoch 45/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12232.0152 - acc: 0.2159\n",
      "Epoch 46/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12222.3030 - acc: 0.2124\n",
      "Epoch 47/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 12213.3051 - acc: 0.2142\n",
      "Epoch 48/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12200.9352 - acc: 0.2137\n",
      "Epoch 49/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12197.0088 - acc: 0.2142\n",
      "Epoch 50/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12186.5255 - acc: 0.2095\n",
      "Epoch 51/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 12173.2449 - acc: 0.2110\n",
      "Epoch 52/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12163.7726 - acc: 0.2066\n",
      "Epoch 53/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12148.8536 - acc: 0.2072\n",
      "Epoch 54/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12140.1346 - acc: 0.2065\n",
      "Epoch 55/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 12131.4033 - acc: 0.2050\n",
      "Epoch 56/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12118.0108 - acc: 0.2059\n",
      "Epoch 57/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12111.0912 - acc: 0.2057\n",
      "Epoch 58/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12094.5133 - acc: 0.2012\n",
      "Epoch 59/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12080.6722 - acc: 0.2039\n",
      "Epoch 60/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 12074.5616 - acc: 0.2050\n",
      "Epoch 61/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12057.7364 - acc: 0.2021\n",
      "Epoch 62/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 12050.2885 - acc: 0.2038\n",
      "Epoch 63/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 12035.0997 - acc: 0.1945\n",
      "Epoch 64/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 12026.3752 - acc: 0.1949\n",
      "Epoch 65/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 12008.3856 - acc: 0.1926\n",
      "Epoch 66/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 11992.6209 - acc: 0.1974\n",
      "Epoch 67/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 11988.2041 - acc: 0.1915\n",
      "Epoch 68/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 11973.8268 - acc: 0.1974\n",
      "Epoch 69/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 11971.7448 - acc: 0.1942\n",
      "Epoch 70/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 11949.2707 - acc: 0.1873\n",
      "Epoch 71/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 11938.0012 - acc: 0.1906\n",
      "Epoch 72/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 11926.0666 - acc: 0.1938\n",
      "Epoch 73/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 11922.2396 - acc: 0.1902\n",
      "Epoch 74/100\n",
      "5536/5536 [==============================] - 0s 50us/step - loss: 11912.0142 - acc: 0.1888\n",
      "Epoch 75/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 11900.0232 - acc: 0.1922\n",
      "Epoch 76/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 11886.6794 - acc: 0.1880\n",
      "Epoch 77/100\n",
      "5536/5536 [==============================] - 0s 50us/step - loss: 11872.0022 - acc: 0.1940\n",
      "Epoch 78/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 11863.1925 - acc: 0.1927\n",
      "Epoch 79/100\n",
      "5536/5536 [==============================] - 0s 56us/step - loss: 11859.1040 - acc: 0.1926\n",
      "Epoch 80/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 11843.8444 - acc: 0.1913\n",
      "Epoch 81/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 11828.2071 - acc: 0.1859\n",
      "Epoch 82/100\n",
      "5536/5536 [==============================] - 0s 48us/step - loss: 11830.4841 - acc: 0.1884\n",
      "Epoch 83/100\n",
      "5536/5536 [==============================] - 0s 59us/step - loss: 11819.9084 - acc: 0.1935\n",
      "Epoch 84/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 11799.9316 - acc: 0.1904\n",
      "Epoch 85/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 11783.1585 - acc: 0.1850\n",
      "Epoch 86/100\n",
      "5536/5536 [==============================] - 0s 55us/step - loss: 11771.0128 - acc: 0.1857\n",
      "Epoch 87/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 11772.0773 - acc: 0.1797\n",
      "Epoch 88/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 11760.8974 - acc: 0.1852\n",
      "Epoch 89/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 11754.3967 - acc: 0.1823\n",
      "Epoch 90/100\n",
      "5536/5536 [==============================] - 0s 57us/step - loss: 11736.2348 - acc: 0.1797\n",
      "Epoch 91/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 11733.4031 - acc: 0.1832\n",
      "Epoch 92/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 11729.5271 - acc: 0.1846\n",
      "Epoch 93/100\n",
      "5536/5536 [==============================] - 0s 53us/step - loss: 11701.3974 - acc: 0.1790\n",
      "Epoch 94/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 11696.0167 - acc: 0.1740\n",
      "Epoch 95/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 11678.3821 - acc: 0.1844\n",
      "Epoch 96/100\n",
      "5536/5536 [==============================] - 0s 51us/step - loss: 11669.2711 - acc: 0.1819\n",
      "Epoch 97/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 11657.6800 - acc: 0.1826\n",
      "Epoch 98/100\n",
      "5536/5536 [==============================] - 0s 52us/step - loss: 11652.6785 - acc: 0.1732\n",
      "Epoch 99/100\n",
      "5536/5536 [==============================] - 0s 49us/step - loss: 11641.8438 - acc: 0.1790\n",
      "Epoch 100/100\n",
      "5536/5536 [==============================] - 0s 54us/step - loss: 11624.4641 - acc: 0.1777\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "history_res = residual_model.fit(residual_X, residual_Y,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         verbose=1,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Sf68ZzdxuRwP"
   },
   "outputs": [],
   "source": [
    "resy_testpred = residual_model.predict(resX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BZbAd6Keyf9i",
    "outputId": "79942245-979e-413b-adbd-577c5f58ce40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10846.635967175682"
      ]
     },
     "execution_count": 150,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(resy_testpred, resy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "du-cQ5tVyj7i",
    "outputId": "5fb276c0-f23d-4ea7-9343-6c10ee16003f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.3938926e-01],\n",
       "       [-9.1224238e-02],\n",
       "       [ 8.3089783e+02],\n",
       "       [ 1.9246199e+04],\n",
       "       [ 2.0349256e+04],\n",
       "       [-3.5304469e-01],\n",
       "       [ 1.6441660e+01],\n",
       "       [-3.6395365e-01],\n",
       "       [-2.7986246e-01],\n",
       "       [ 7.2726001e+03],\n",
       "       [ 1.6749376e+01],\n",
       "       [ 1.8580405e+02],\n",
       "       [-3.2087916e-01],\n",
       "       [ 4.1314194e+03],\n",
       "       [-4.0914243e-01],\n",
       "       [-3.0935699e-01],\n",
       "       [ 3.3513664e+04],\n",
       "       [ 9.9610068e+03],\n",
       "       [ 1.4283991e+04],\n",
       "       [ 3.1937469e+01]], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resy_testpred[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "WSUy_Wu1ypCL",
    "outputId": "1ee83685-fad4-4036-9645-9f4d162b690a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00, -3.61563242e+04,  9.73794922e-01,\n",
       "        3.04422574e+05, -1.62902355e+00,  1.20000000e+01,  1.34611055e+02,\n",
       "        0.00000000e+00,  2.40000000e+01,  8.74454269e+01, -7.42779541e+02,\n",
       "        0.00000000e+00,  1.62185856e+04, -2.65790272e+00,  1.48800000e+03,\n",
       "        9.53010547e+03,  3.59818354e+02,  5.81826431e+03,  3.54207712e+03])"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resy_test[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HEuOC3IzytFi"
   },
   "outputs": [],
   "source": [
    "word_vector_test = D_test.iloc[:,14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hVwyE5Xz2uN9"
   },
   "outputs": [],
   "source": [
    "residual_test_pred = residual_model.predict(word_vector_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bzUF11I_2vi3",
    "outputId": "65c63981-2a16-4bc4-e14a-84f67382e9ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1)"
      ]
     },
     "execution_count": 166,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vT6Zf60d3JoH"
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred + residual_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZqB8wO3h3Pyo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hWCZzGTa3iUb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "data explore.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
